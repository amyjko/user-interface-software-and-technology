If you don't know the history of computing, it's easy to overlook the fact that all of the interfaces that people used to control computers before the graphical user interface were _programming_ interfaces. But that's no longer true; why then talk about programming in a book about user interface technology? Two reasons: programmers are users too, and perhaps more importantly, user interfaces quite often have programming-like features embedded in them. For example, spreadsheets aren't just tables of data; they also tend to contain formulas that perform calculations on that data. Or, consider the increasingly common trend of smart home devices: to make even basic use of them, one must often write basic programs to automate their use (e.g., turning on a light when one arrives home, or defining the conditions in which a smart doorbell sends a notification). Programming interfaces are therefore far from niche: they are the foundation of how all interfaces are built and increasingly part of using interfaces to control and configure devices.

But it's also important to understand programming interfaces in order to understand why interactive interfaces -- the topic of our [next chapter|interactive] -- are so powerful. This chapter won't teach you to code if you don't already know how, but it will give you the concepts and vocabulary to understand the fundamental differences between these two interface paradigms, when one might be used over the other, and what challenges each poses to people using computers.

|programming-spreadsheet.png|A screenshot of Excel, showing a basic sum of column of numbers.|What makes spreadsheets a programming interface?|[Excel Easy|https://www.excel-easy.com/examples/sum.html]|

# Properties of programming interfaces

In some ways, programming interfaces are like any other kind of user interface: they take input, present output, have affordances and signifiers, and present a variety of gulfs of evaluation and execution. They do this by using a collection of tools, which make up the "interface" a programmer users to write computer programs:

* *Programming languages*. These are interfaces that take computer programs, translate them into instructions that a computer can execute, and then execute them. Popular examples include languages like `Python` (depicted at the beginning of this chapter) and `JavaScript` (which is used to build interactive web sites) but also less well-known languages such as [`Scratch`|http://scratch.mit.edu], which youth often use to create interactive 2D animations and games, or `R`, which statisticians and data scientists use to analyze data.

* *Editors*. These are editors, much like word processes, that programmers use to read and edit computer programs. Some languages are editor-agnostic (e.g., you can edit Python code in any text editor), whereas others come with dedicated editors (e.g., Scratch programs can only be written with the Scratch editor).

These two tools, along with many other optional tools to help streamline the many challenges that come with programming, are the core interfaces that people use to write computer programs. Programming, therefore, generally involves reading and editing code in an editor, and repeatedly asking a programming language to read the code to see if there are any errors in it, and then executing the program to see if it has the intended behavior. It often doesn't, which requires people to debug their code, finding the parts of it leading the program to misbehave. This _edit-run-debug_ cycle is the general experience of programming, slowly writing and revising a program until it behaves as intended (much like slowly sculpting a sculpture, writing an essay, or painting a painting).

While it's obvious this is different from using a website or mobile application, it's helpful to talk in more precise terms about exactly how it is different. In particular, there are three attributes in particular that make an interface a programming interface<blackwell02>: 

* *No direct manipulation*. In interactive interfaces like a web page or mobile app, concrete physical actions by a user on some data -- moving a mouse, tapping on an object -- result in concrete, immediate feedback about the effect of those actions. For example, dragging a file from one folder to another immediately moves the file, both visually, and on disk; selecting and dragging some text in a word processor moves the text. In contrast, programming interfaces offer no such direct manipulation: one _declares_ what a program should do to some data, and then only after the program is executed does that action occur (if it was declared correctly). Programming interfaces therefore always involve _indirect_ manipulation, describing.

* *Use of notation*. In interactive interfaces, most interactions involve concrete representations of data and action. For example, a web form has controls for entering information, and a submit button for saving it. But in programming interfaces, references to data and actions always involves using some programming language notation to refer to data and actions. For example, to open a new folder in the Unix or Linux operating systems, one has to type `cd myFolder`, which is a particular notation for executing the `cd` command (standing for "change directory" and specifying which folder to navigate to (`myFolder)`. Programming interfaces therefore always involve specific rules for describing what a user wants the computer to do.

* *Use of abstraction*. An inherent side effect of using notations is that one must also use abstractions to refer to a computer's actions. For example, when one uses the Unix command line program `mv file.txt ../` (which moves `file.txt` to its parent directory), they are using a command, a file name, and a symbolic reference `../`  to refer to the parent directory of the current directory. Commands, names, and relative path references are all abstractions in that they all abstractly _represent_ the data and actions to be performed. Visual icons in a graphical user interface are also abstract, but because they support direct manipulation, such as moving files from one folder to another, they can be treated as concrete objects. Not so with programming interfaces, where everything is abstract.

Clearly, textual programming languages such as the popular `Python`, `Java` or `JavaScript` fit all all of the descriptions above. But what about programming interfaces like Excel spreadsheets? Thinking through each of the properties above, we see spreadsheets have many of the same properties. While one can directly manipulate data in cells, there are no direct manipulation ways to use formulas: those must be written in a notation, the Excel programming language. And the notation invokes abstract ideas like `sum`ing rows and columns. Spreadsheets blur the line between programming and interactive interfaces, however, by immediately executing formulas as soon as they are edited, recomputing everything in the spreadsheet. This gives a a similar sense of immediacy as direct manipulation, even though it is not direct.

Another interesting case is that of chat bots, including those with pre-defined responses like in help tools and Discord, as well as more sophisticated ones like ChatGPT and others that use large language models (LLMs) trained on all of the internet's public documents. Is writing prompts a form of programming? They seem to satisfy all of the properties above. They do not support direct manipulation, because they require a descriptions of commands. They do use notation, just natural language instead of a formal notation like a programming language. And they definitely use abstraction, in that they rely on sequences of symbols that represent abstract ideas (otherwise known as _words_) to refer to what one wants a computer to do. If we accept the definition above, then yes, chat bots and voice interfaces are programming interfaces, and may entail similar challenges as more conventional programming interfaces do. And we shouldn't be surprised: all large-language models do (along with more rudimentary approaches to responding to language) is take a series of symbols as inputs, and generate a series of symbols in response, just like any other function in a computer program.

|programming-scratch.png|Four children standing around a laptop writing Scratch code.|Learning programming interfaces requires patience, persistence, and problem solving.|MIT|

# Learning programming interfaces

Because programming interfaces are interfaces, they also have ~gulfs of execution~gulfofex and ~evaluation~gulfofeval as well. Their gulfs are just much larger because of the lack of direct manipulation, the use of notation, and the centrality of abstraction. All of these create a greater distance between someone's goal, which often involves delegating or automating some task with code.

At the level of writing code, the gulfs of execution are immense. Imagine, for example, you have an idea for an mobile app, and are starting at a blank code editor. To have any clue what to type, people must know 1) a programming language, including its notation, and the various rules for using the notation correctly; 2) basic concepts in computer science about data structures and algorithms, _and_ 3) how to operate the code editor, which may have many features designed assuming all of this prior knowledge. Its rare, therefore, that a person might just poke around an editor or programming language to learn how it works. Instead, learning usually requires good documentation, perhaps some tutorials, and quite possibly a teacher and an education system. The same can be true of complex interactive interfaces (e.g., learning [Photoshop|https://www.adobe.com/products/photoshop/]), but at least in the case of interactive interfaces, the functional affordances is explicitly presented in menus. With programming interfaces, affordances are invisible.

Once one has some code and executes it, programming interfaces pose similarly large gulfs of evaluation. Most programs start off defective in some way, not quite doing what a person intended: when the program misbehaves, or gives an error message, what is a programmer to do? To have any clue, people lean on their knowledge of programming language to interpret error messages, carefully analyze their code, and potentially use other tools like debuggers to understand where their instructions might have gone wrong. These debugging skills are similar to the troubleshooting skills required by in interactive interfaces (e.g., figuring out why Microsoft Word keeps autocorrecting something), but the solutions are often more than just unchecking a checkbox: they may involve revising or entirely rewriting a part of a program.

If these two gulfs were not enough, modern programming interfaces have come to introduce new gulfs. These new gulfs primarily emerge from the increasing use of ~APIs~api to construct applications by reusing other people's code. Some of the gulfs that reuse impose include<ko04>:
 
* *Design gulfs*. APIs make some things easy to create, while making other things hard or impossible. This gulf of execution requires programmers to know what is possible to express.
* *Selection gulfs*. APIs may offer a variety of functionality, but it's not always clear what functionality might be relevant to implementing something. This gulf of execution requires programmers to read documentation, scour web forums, and talk to experts to find the API that might best serve their need.
* *Use gulfs*. Once someone find an API that might be useful for creating what they want, they have to learn how to _use_ the API. Bridging this gulf of execution might entail carefully reading documentation, finding example code, and understanding how the example code works.
* *Coordination gulfs*. Once someone learns how to use one part of an API, they might need to use it in coordination with another part to achieve the behavior they want. Bridging this gulf of execution might require finding more complex examples, or creatively tinkering with the API to understand its limitations.
* *Understanding gulfs*. Once someone has written some code with an API, they may need to debug their code, but without the ability to see the API's own code. This poses gulfs of evaluation, requiring programmers to carefully interpret and analyze API behavior, or find explanations of its behavior in resources or online communities. 

These many gulfs of execution and evaluation mean have two implications: 1) programming interfaces are hard to learn and 2) designing programming interfaces is about more than just programming language design. It's about supporting a whole ecosystem of tools and resources to support learning.

If we return to our examination of chat bots, we can see that they also have the same gulfs. It's not clear what one has to say to a chat bot to get the response one wants. Different phrases have different effects, in unpredictable ways. And once one does say something, if the results are unexpected, it's not clear why the model produced the results it did, and what action needs to be taken to produce a different response. "Prompt engineering" may just be modern lingo for "programming" and "debugging", just without all of the usual tools, documentation, and clarity of how to use them to get what you want.

# Simplifying programming interfaces

Given all of the difficulties that programming interfaces pose, its reasonable to wonder why we use them at all, instead of just creating interactive interfaces for everything we want computers to do. Unfortunately, that simply isn't possible: creating new applications requires programming; customizing the behavior of software to meet our particular needs requires programming; and automating any task with a computer requires programming. Therefore, many researchers have worked hard to reduce the gulfs in programming interfaces as much as possible, to enable more people to succeed in using them.

While there is a vast literature on programming languages and tools, and much of it focuses on bridging gulfs of execution and evaluation, in this chapter, we'll focus on the contributions that HCI researchers have made to solving these problems, as they demonstrate the rich, under-explored design space of ways that people can program computers beyond using general purpose languages. Much of this work can be described as supporting ~end-user programming~eup, which is any programming that someone does as a means to accomplishing some other goal<ko11>. For example, a teacher writing formulas in a spreadsheet to compute grades, a child using [Scratch|https://scratch.mit.edu/] to create an animation, or a data scientist writing a Python script to wrangle some data -- none of these people are writing code for the code itself (as professional software engineers do), they're writing code for the output their program will produce (the teacher wants the grades, the child wants the animation, the data scientist wants the wrangled data).
 
|sikuli.png|A screenshot of the Sikuli system, showing a loop that waits for a particular image to appear on a map before showing a popup that says the bus has arrived|Sikuli<yeh09>|Tom Yeh|

This vast range of domains in which programming interfaces can be used has lead to an abundance of unique interfaces. For example, several researchers have explored ways to automate interaction with user interfaces, to take repetitive tasks and automate them. One such system is Sikuli (above), which allows users to use screenshots of user interfaces to write scripts that automate interactions<yeh09>. Similar systems have used similar ideas to enable users to write simple programs to automate web tasks. CoScripter<leshed08> allowed a user to demonstrate an interaction with an interface, which generated a program in a natural-language-like syntax that could then be executed to replay that action. CoCo<lau10> allowed a user to write a command like "get road conditions for highway 88," which the system then translates into operations on a website using the user's previous browsing history and previously recorded web scripts. Two related contributions used the metaphor of a command line interface for the web and desktop applications: one recommended commands based on natural language descriptions of tasks<miller08> and the other used a "sloppy" syntax of keywords<little06>. All of these ideas bridge the gulf of execution, helping a user express their goal in terms they understand such as demonstrating an action, selecting part of an interface, or describing their goal, then having the system translate these into an executable program.

|vega.png|A screenshot of a Vega program, which declares a scatterplot with brushing and linking feature. |Vega<satyanarayan14>|Satyanarayan et al.|

Another major focus has been supporting people interacting with data. Some systems like Vega above have offered new programming languages for declaratively specifying data visualizations<satyanarayan14>. Others have provided programmatic means for wrangling and transforming data with interactive support for previewing the effects of candidate transformation programs<guo11,mayer15>. One system looked at interactive ways of helping users search and filter data with regular expressions by identifying examples of outlier data<miller01>, whereas other systems have helped translate examples of desired database queries into SQL queries<abouzied12>. Again, a major goal of all of these systems is to help bridge the gulf of evaluation between a user's goal and the program necessary to achieve it.
 
|mavo.png|A screenshot of the Mavo system, showing a simple HTML body with attribute annotations and the corresponding to do list application the annotations specify|Mavo<verou16>|Verou et al.|
 
Some systems have attempted to support more ambitious automation, empowering users to create entire applications that better support their personal information needs. For example, many systems have combined spreadsheets with other simple scripting languages to enable users to write simple web applications with rich interfaces, using the spreadsheet as a database<chang14,benson14>. Other systems like Mavo above and Dido have encapsulated the entire application writing process to just editing HTML by treating HTML as a specification for both the layout of a page and the layout of data<verou16,karger09>. An increasing number of systems have explored home automation domains, finding clear tradeoffs between simplicity and expressiveness in rule-based programs<brich17>.
 
|hands.jpg|A screenshot of the Hands system, showing a program that animates bees, and a card with bee properties.|Hands<pane02>|Pane and Myers|

Perhaps the richest category of end-user programming systems are those supporting the creation of games. Hundreds of systems have provided custom programming languages and development environments for authoring games, ranging from simple game mechanics to entire general purpose programming languages to support games<kelleher05>. For example, the system above, called Hands, carefully studied how children express computation with natural language, and designed a programming language inspired by the computational ideas inherent in children's reasoning about game behavior<pane02>. Other systems, most notably Gamut<mcdaniel97>, used a technique called _programming by demonstration_, in which users demonstrate the behavior they want the computer to perform, and the computer generalizes that into a program that can be executed later on a broader range of situations than the original demonstration. Gamut was notable for its ability to support the construction of an _entire_ application by demonstration, unlike many of the systems discussed above, which only partially used demonstration.

|https://www.youtube.com/embed/fP8swbzeDuY|The Whyline for Alice|The Whyline lets people ask "why" questions about a program's behavior.<ko04b>|Amy J. Ko|

Most innovations for programming interfaces have focused on bridging the gulf of execution. Fewer systems have focused on bridging gulfs of evaluation by supporting, testing, and debugging behaviors a user is trying to understand. One from my own research was a system called the Whyline<ko04> (see the video above), which allowed users to ask "why" and "why not" questions when their program did something they didn't expect, bridging a gulf of evaluation. It identified questions by scanning the user's program for all possible program outputs, and answered questions by precisely trying the cause of every operation in the program, reasoning backwards about the chain of causality that caused an unwanted behavior or prevented a desired behavior. More recent systems have provided similar debugging and program comprehension support for understanding web pages<burg15,hibschman15>, machine learned classifiers<patel10,kulesza09>, and even embedded systems that use a combination of hardware and software to define a user interface<strasnick17,mcgrath17>.
 
---

One way to think about all of these innovations is as trying to bring all of the benefits of interactive interfaces -- direct manipulation, no notation, and concreteness -- to notations that inherently don't have those properties, by augmenting programming _environments_ with these features. This work blurs the distinction between programming interfaces and interactive interfaces, bringing the power of programming to broader and more diverse audiences. But the work above also makes it clear that there are limits to this blurring: no matter how hard we try, _describing_ what we want rather than _demonstrating_ it directly, always seems to be more difficult, and yet more powerful.
In the last chapter, we discussed how interfaces communicate data, content, and animations on two-dimensional screens. However, human visual perception is not limited to two dimensions: we spend most of our lives interacting in three-dimensional space. The prevalence of flat screens on our computers, smartwatches, and smartphones is primarily due to limitations in the technology. However, with advances in 3D graphics, display technology, and sensors, we’ve been able to build virtual and augmented reality systems that allow us to interact with technology in immersive 3D environments. Moving through physical environments allows us to reconstruct mental models of a space and its contents, allowing us to interface with the physical world. 

Computing has brought opportunities to create dynamic virtual worlds, but with this has come the need to properly map those virtual worlds onto our ability to perceive our environment. This poses a number of interdisciplinary research challenges that has seen renewed interest over the last decade. In this chapter, we’ll explore some of the current research trends and challenges in building virtual reality and augmented reality systems. 

# Virtual reality

The goal of VR has always been consistent with these other forms of virtual realities: *immersion*. Graphical user interfaces, while not intended to be immersive, can result in a degree of immersive flow when well designed. Movies, when viewed in movie theaters, are already quite good at achieving immersion. Virtual reality aims for _total_ immersion, while integrating interactivity. Moreover, the user should feel a sense of *presence*. Not only should a VR system make you perceive the visual sensations of being in a virtual environment, but it should make you feel that you’re actually engaged and psychologically present in the environment. 

One of the core concepts behind VR is enabling the perception of 3D graphics by presenting a different image to each eye using stereo displays. By manipulating subtle differences between the image in each eye, it is possible to make objects appear at a particular depth. An early example using this technique was the View-Master, a popular toy developed in the 1930s that showed a static 3D scene using transparencies on a wheel. 

|viewmaster.jpg|A View-Master toy that showed a static 3D scene using stereo images|A View-Master toy which let’s users view static images in 3D, using a similar principle to today’s VR headsets|[Wikipedia|https://en.wikipedia.org/wiki/View-Master#/media/File:View-Master_Model_E.JPG]|

One of the first virtual reality systems to be connected to a computer was Ivan Sutherland's "Sword of Damocles", shown below, and created in 1968. With his student Bob Sproull, the headmounted display they created was far too heavy for someone to wear. The computer generated basic wireframe rooms and objects. This early work was pure proof of concept, trying to imagine a device that could render _any_ interactive virtual world. 

|sutherland-sword-of-damocles.jpg|A picture of someone using Ivan Sutherland's first virtual reality headset, mounted from the ceiling.|Ivan Sutherland's Sword of Damocles, the first virtual reality headset.|Unknown|

In the 1990s, researchers developed CAVE systems (cave automatic virtual environments) to explore fully immersive virtual reality. These systems used multiple projectors with polarized light and special 3D glasses to control the image seen by each eye. After careful calibration, a CAVE user will perceive a fully 3D environment with a wide field of view. 

|cave.jpg|A View-Master toy that showed a static 3D scene using stereo images|A CAVE allows users to explore a virtual environment without a head-mounted display.|[Wikipedia|https://upload.wikimedia.org/wikipedia/commons/6/6d/CAVE_Crayoland.jpg]|

Jaron Lanier, one of the first people to write and speak about VR in its modern incarnation, originally viewed VR as an empathy machine capable of helping humanity have experiences that they could not have otherwise. In [an interview with the Verge in 2017|https://www.theverge.com/2017/12/8/16751596/jaron-lanier-dawn-of-the-new-everything-vr-interview], he lamented how much of this vision was lost in the modern efforts to engineer VR platforms:

"
If you were interviewing my 20-something self, I'd be all over the place with this very eloquent and guru-like pitch that VR was the ultimate empathy machine--that through VR we’d be able to experience a broader range of identities and it would help us see the world in a broader way and be less stuck in our own heads. That rhetoric has been quite present in recent VR culture, but there are no guarantees there. There was recently this kind of ridiculous fail where \[Mark\] Zuckerberg was showing devastation in Puerto Rico and saying, "This is a great empathy machine, isn't it magical to experience this?" While he’s in this devastated place that the country's abandoned. And there's something just enraging about that. Empathy should sometimes be angry, if anger is the appropriate response.
" Jaron Lanier

While modern VR efforts have often been motivated by ideas of empathy, most of the research and development investment has focused on fundamental engineering and design challenges over content:

* Making hardware light enough to fit comfortably on someone's head
* Untethering a user from cables, allowing freedom of movement
* Sensing movement at a fidelity to mirror movement in virtual space
* Ensuring users don't hurt themselves in the physical world because of total immersion
* Devising new forms of input that work when a user cannot even see their hands
* Improving display technology to reduce simulator sickness
* Adding haptic feedback to further improve as sense of immersion

Most HCI research on these problems has focused on new forms of input and output. For example, researchers have considered ways of using the backside of a VR headset as touch input<gugenheimer16a,lyons16,hsieh16,whitmire17>, and the use of pens or other tools for sketching in VR<arora17>. Researchers have also investigated the use of haptics to allow the user to feel aspects of the virtual environment. Recent examples of advances in haptics include placing a flywheel on the headset to simulate inertia<gugenheimer16b>, using ultrasound to deliver feedback to the hands<long14>, or rendering shear forces on the fingertips with wearable devices<schorr17>. Some of these approaches have been applied to non-virtual immersive experiences for blind people, providing vibrations and spatialized audio on a cane<siu20>. Another class of haptic devices rely on handheld controllers, adding the ability to render the sensation of weight<zenner17>, grasping an object<choi17>, or exploring virtual 3D surfaces<benko16>, as in this demo:

|https://www.youtube.com/embed/KhbUg3_3T0I|NormalTouch and TextureTouch: High-fidelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers|An approach to providing haptic feedback with an extrudable surface.|Benjko et al.<benko16>|

Some approaches to VR content have focused on somewhat creepy ways of exploiting immersion to steer users’ attention or physical direction of motion. Examples include using a haptic reflex on the head to steer users<kon17>, or this system, which nudges humans to walk in a direction under someone else's control by shifting the user's field of view<ishii16>:

|https://www.youtube.com/embed/2fclyYxAEzs|Optical Marionette: Graphical Manipulation of Human's Walking Direction|Controlling another human through VR.|Ishii et al.<ishii16>|

Other more mainstream applications have included training simulations and games<zyda05> and some educational applications<pan06>, but designers are still very much learning about how to exploit the unique properties of the medium in search of killer apps. Beyond gaming, the industry has also explored VR as a storytelling medium. For example, [Henry|https://www.oculus.com/story-studio/films/henry/] is a short story designed by Oculus Story Studio that won the first Emmy award for a VR short. Because of VR’s roots in the video game industry, designers and storytellers have made use of game engines for storytelling, but custom design tools for storytelling will likely evolve over time. 

# Augmented reality

Augmented reality (AR), in contrast to virtual reality, does not aim for complete immersion in a virtual environment, but to alter reality to support human augmentation. This vision for AR goes back to Ivan Sutherland (of [Sketchpad|history]) in the 1960's, who dabbled with head mounted displays for augmentation. Only in the late 1990's did the hardware and software sufficient for augmented reality begin to emerge, leading to research innovation in displays, projection, sensors, tracking, and, of course, interaction design<azuma01>. This has culminated in a range of commercially available techniques and toolkits for AR, most notably Apple's [ARKit|https://developer.apple.com/arkit/], which is the most ubiquitous deployment of a mixed reality platform to date. Other notable examples are [ARCore|https://developers.google.com/ar/], which emerged from Google's [Project Tango|https://en.wikipedia.org/wiki/Tango_(platform)], and Microsoft's [Hololens|https://www.microsoft.com/en-us/hololens]. All are heavily informed by decades of academic and industry research on augmented reality. 

Research problems in AR are similar to those in VR, but with a set of additional challenges and constraints. For one, the requirements of tracking accuracy and latency are much stricter, since any errors in rendering virtual content will be made more obvious by the physical background. For head-mounted AR systems, the design of displays and optics are challenging, since they must be able to render content without obscuring the user’s view of the real world. When placing virtual objects in a physical space, researchers have looked at how to match the lighting of the physical space so virtual objects look believable<lensing12,richtertrummer16>. HCI contributions to AR have focused primarily on how to coordinate input and output in mixed reality applications. For example, one application envisioned a method for remote participants using VR, with others using AR to view the remote participant as teleported into a target environment<ortsescolano16>:

|https://www.youtube.com/embed/o00mn1XbClg|Holoportation: Virtual 3D Teleportation in Real-time|3D teleportation via augmented reality|Orts-Escolano et al<ortsescolano16>|

Interaction issues here abound. How can users interact with the same object? What happens if the source and target environments are not the same physical dimensions? What kind of broader context is necessary to support collaboration? Some research has tried to address some of these lower-level questions. For example, one way for remote participants to interact with a shared object is to make one of them virtual, tracking the real one in 3D<oda15>. Another project provided an overview of the physical space and the objects in it, helping to facilitate interaction with virtual objects<bell02>. Research has also dealt with how to annotate physical scenes without occluding important objects, requiring some notion of what is and is not important in a scene<bell01>. Other researchers have looked at how to use physical objects to interact with virtual characters.<cimen17>.

Some interaction research attempts to solve lower-level challenges. For example, many AR glasses have narrow field of view, limiting immersion, but adding further ambient projections can widen the viewing angle<benko15>. There are also other geometric limitations in scene tracking, which can manifest as *registration errors* between the graphics and the physical world, leading to ambiguity in interaction with virtual objects. This can be overcome by propagating geometric uncertainty throughout the scene graph of the rendered scene, improving estimates of the locations of objects in real time<coelho05>. 

---

From smartphone-based VR, to the more advanced mixed reality visions blending the physical and virtual worlds, designing interactive experiences around 3D output offers great potential for new media, but also great challenges in finding meaningful applications and seamless interactions. Researchers are still hard at work trying to address these challenges, while industry forges ahead on scaling the robust engineering of practical hardware. 

There are also many open questions about how 3D output will interact with the world around it:

* How can people seamlessly switch between AR, VR, and other modes of use while performing the same task?
* How can VR engage groups when not everyone has a headset?
* What activities are VR and AR suitable for? What tasks are they terrible for?

These and a myriad of other questions are critical for determining what society chooses to do with AR and VR and how ubiquitous it becomes.
In a rapidly evolving field of augmented reality, driverless cars, and rich social media, is there any more mundane way to start a book on user interfaces than history? Probably not. And yet, for a medium that is invented entirely from imagination, the history of user interfaces is reminder that the interactive world we experience today could have been different. Files, keyboards, mice, and touchscreens--all of these concepts were invented to solve very real challenges with interacting with computers, and but for a few important people and places in the 20th century, we might have invented entirely different ways of interacting with computers<card86>.

Computing began as an idea: Charles Babbage, a mathematician, philosopher, and inventor in London in the early 19th century, began imagining a kind of mechanical machine that could automatically calculate mathematical formulas. He called it the "Analytical Engine", and conceived of it as a device that would encode instructions for calculations. Taking arbitrary data encoded as input on punch cards, this engine would be able to quickly and automatically calculate formulas much faster than people. And indeed, at the time, people were the bottleneck: if a banker wanted a list of numbers to be added, they needed to hire a computer--a person that quickly and correctly performed arithmetic--to add them. Baggage's "Analytical Engine" promised to do the work of (human) computers much more quickly and accurately. Babbage later mentored Ada Lovelace, the daughter of a wealthy aristocrat; Lovelace loved the beauty of mathematics, and was enamored by Babbage's vision, publishing the first algorithms intended to executed by such a machine. Thus began the age of computing, shaped largely by values of profit but also the beauty of mathematics.

|punchcards.jpg|A photograph of a woman using a punchcard machine.|Long before computers were electronic, paper punchcards encoded both code and data, and were therefore the first computer interfaces.|Public domain|

While no one ever succeeded in making a _mechanical_ analytical engine, one hundred years later, electronic technology made _digital_ ones possible<gleick11>. But Babbage's vision of using punch cards for input remained intact: if you used a computer, it meant constructing programs out of individual machine instructions like add, subtract, or jump, and encoding them onto punch cards to be read by a mainframe. And rather than being driven by a desire to lower costs and increase profit in business, these computers were instruments of war, decrypting German secret messages and more quickly calculating ballistic trajectories.

As wars came to a close, some found the vision of computers as business and war machines too limiting. [Vannevar Bush|https://en.wikipedia.org/wiki/Vannevar_Bush], a science administrator who headed the U.S. Office of Scientific Research and Development, had a much bigger vision. He wrote a 1945 article in _The Atlantic Monthly_, called "As We May Think." In it, he envisioned a device called the "Memex" in which people could store all of their information, including books, records, and communications, and access them with speed and flexibility<bush45>. Notably, Bush imagined the Memex as an "enlarged intimate supplement to one's memory." This vision, far from being driven by defense or business, was driven by a dream of human augmentation, expanding our abilities as a species to learn, think, and understand through information.

|memex.png|A line drawing of a machine with several displays, a keyboard, and a computer inside a desk, retriving and calculating.|The memex, as depicted in Bush's article, _As We May Think_<bush45>|Public domain|

Here is how Bush described the Memex:

"
Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, "memex" will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory... Wholly new forms of encyclopedias will appear, ready made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified. The lawyer has at his touch the associated opinions and decisions of his whole experience, and of the experience of friends and authorities. The patent attorney has on call the millions of issued patents, with familiar trails to every point of his client's interest. The physician, puzzled by a patient's reactions, strikes the trail established in studying an earlier similar case, and runs rapidly through analogous case histories, with side references to the classics for the pertinent anatomy and histology. The chemist, struggling with the synthesis of an organic compound, has all the chemical literature before him in his laboratory, with trails following the analogies of compounds, and side trails to their physical and chemical behavior.
" Vannevar Bush<bush45>

Does this remind you of anything? The internet, hyperlinks, Wikipedia, networked databases, social media. All of it is there in this prescient description of human augmentation, including rich descriptions of the screens, levers, and other controls for accessing information.

J.C.R. Licklider, an American psychologist and computer scientist, was fascinated by Bush's vision, writing later in his "Man-Computer Symbiosis"<licklider60>.

"
...many problems that can be thought through in advance are very difficult to think through in advance. They would be easier to solve, and they could be solved faster, through an intuitively guided trial-and-error procedure in which the computer cooperated, turning up flaws in the reasoning or revealing unexpected turns in the solution. Other problems simply cannot be formulated without computing-machine aid. Poincare anticipated the frustration of an important group of would-be computer users when he said, "The question is not, 'What is the answer?' The question is, 'What is the question?'" One of the main aims of man-computer symbiosis is to bring the computing machine effectively into the formulative parts of technical problems.
"

He then talked about visions of a "thinking center" that "_will incorporate the functions of present-day libraries together with anticipated advances in information storage and retrieval _," connecting individuals and computers through desk displays, wall displays, speech production and recognition, and other forms of artificial intelligence. In his role in the United States Department of Defense Advanced Research Projects Agency and as a professor at MIT, Licklider funded and facilitated the research that eventually led to the internet and the graphical user interface.

One person that Bush and Licklider's ideas influenced was MIT computer scientist [Ivan Sutherland|https://en.wikipedia.org/wiki/Ivan_Sutherland]. He followed this vision of human augmentation by exploring interactive sketching on computers, working on a system called Sketchpad for his dissertation<sutherland63>. Sketchpad, seen in the video below, allowed drawing of segments, arcs, and constraints between shapes. Shapes could be transformed, resized, repositioned, and clipped, and a notion of windowing allowed zooming and panning.  The entire interactive experience was stylus-based and the implementation paradigm was object-based. This was the first system to ever demonstrate an interactive dialog with a computer, rather than a "batch processing" programming-based interaction. You can see Sketchpad in action in the video below.

|https://www.youtube.com/embed/6orsmFndx_o|Ivan Sutherland's Sketchpad demonstration|Sketchpad was one of the first demonstrations of an interactive interface.|Amy J. Ko|

Sketchpad and Bush and Licklider's article inspired Douglas Engelbart to found the [Augmentation Research Center|https://en.wikipedia.org/wiki/Augmentation_Research_Center] at the Stanford Research Institute (SRI) in the early 1960's.  Over the course of about six years, with funding from NASA and the U.S. Defense Department's Advanced Research Projects Agency (known today as DARPA), Engelbart and his team prototyped the "oN-Line System" (or the NLS), which attempted to engineer much of Bush's vision. NLS had networking, windows, hypertext, graphics, command input, video conferencing, the computer mouse, word processing, file version control, text editing, and numerous other features of modern computing. Engelbart himself demoed the system to a live audience in what is often called "The Mother of all Demos." You can see the entire demonstration in the below above.

|https://www.youtube.com/embed/yJDv-zdhzMY|Engelbart's "mother of all demos"|Engelbart's demo demonstrated everything from user interfaces and file systems to networking|SRI|

Engelbart's research team eventually disbanded and many of them ended up at the Xerox Palo Alto Research Center (Xerox PARC). Many were truly inspired by the demo and wanted to use the freedom Xerox had given them to make the NLS prototype a reality. One of the key members of this team was [Alan Kay|https://en.wikipedia.org/wiki/Alan_Kay], who had worked with Ivan Sutherland and seen Engelbart's demo. Kay was interested in ideas of objects, object-oriented programming, and windowing systems, and created the programming language and environment Smalltalk. He was a key member of a team at PARC that developed the Alto in the mid 1970's. The Alto included the first operating system based on a graphical user interface with a desktop metaphor, and included WYSIWYG word processing, an email client, a vector graphics editor, a painting program, and a multi-player networked video game.

|https://www.youtube.com/embed/9H79_kKzmFs|Xerox Alto Demo|The Alto brought together many research ideas|Amy J. Ko|

Now, these were richly envisioned prototypes, but they were not products. You couldn't buy an Alto at a store. This changed when Steve Jobs visited Xerox PARC in 1979, where he saw a demo of the Alto's GUI, its Smalltalk-based programming environment, and its networking. Jobs was particularly excited about the GUI, and recruited several of the Xerox PARC researchers to join Apple, leading to the Lisa and later Macintosh computers, which offered the first mass-market graphical user interfaces. Apple famously marketed the Macintosh in its 1984 advertisement, framing the GUI the first salvo in a war against Big Brother, a reference to the book, _[1984|https://en.wikipedia.org/wiki/Nineteen_Eighty-Four]_.

|https://www.youtube.com/embed/VtvjbmoDx-I|Apple's Macintosh Commercial|The Mac brough the Alto's idea to market.|Apple|

As the Macintosh platform expanded, it drew many innovators, including many Silicon Valley entreprenuers with ideas for how to make personal computers even more powerful. This included ideas like presentation software such as [PowerPoint|https://spectrum.ieee.org/tech-history/cyberspace/the-improbable-origins-of-powerpoint], which grew out of the same Xerox PARC vision of what-you-see-is-what-you-get text and graphics manipulation.

Since the release of the Macintosh, companies like Apple, Microsoft, and now Google have driven much of the engineering of user interfaces, deviating little from the original visions inspired by Bush, Licklider, Sutherland, Engelbart, and Kay. But governments and industry continued to harvest basic research for new paradigms of interaction, including the rapid proliferation of capacitive touch screens in the early 2000's in smartphones<myers98>. Around the same time as the release of the Macintosh, many computer scientists who had worked in computer graphics (a field started by Sutherland) spun off a new conference in 1988, the [ACM Symposium on User Interface Software and Technology|https://uist.acm.org]. This community brought together researchers interested in not only the useful _output_ that computers could produce, but also the novel forms of _input_, and the interaction paradigms that combinations of input and output could produce. This community has since worked alongside industry, establishing the basic paradigms for graphical user interfaces, while continuing to invent new ways of interacting with computers, including paradigms such as augmented reality, touchscreens, more efficient text entry, and animation<myers00,vandam97,weiser91>.

Reflecting on this history, one of the most remarkable things is how powerful one vision was to catalyze an entire world's experience with computers. Is there something inherently fundamental about networked computers, graphical user interfaces, and the internet that was inevitable? Or if someone else had written an alternative vision for computing, would we be having different interactions with computers and therefore different interactions with each other through computing? And is it still possible to imagine different futures of interactive computing that will shape the future yet? This history and these questions remind us that nothing about our interactions with computing is necessarily "true" or "right": they're just ideas that we've collectively built, shared, and learned--and they can change. In the coming chapters, we'll uncover what _is _ fundamental about user interfaces and explore alternative visions of interacting with computers that may require new fundamentals.
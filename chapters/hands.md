Thus far, we have discussed two forms of input to computers: [pointing|pointing] and [text entry|text]. Both are mostly sufficient for operating most forms of computers. But, as we discussed in our chapter on [history|history], interfaces have always been about augmenting human ability and cognition, and so researchers have pushed far beyond pointing and text to explore many new forms of input. In this chapter, we focus on the use of hands to interact with computers, including *touchscreens*, *pens*, *gestures*, and *hand tracking*. 

One of the central motivations for exploring hand-based input came from new visions of interactive computing. For instance, in 1991, Mark Weiser, who at the time was head of the very same Xerox PARC that led to the first GUI, wrote in Scientific American about a vision of _ubiquitous computing_.<weiser91> In this vision, computing would disappear, become invisible, and become a seamless part of everyday tasks:

"
Hundreds of computers in a room could seem intimidating at first, just as hundreds of volts coursing through wires in the walls did at one time. But like the wires in the walls, these hundreds of computers will come to be invisible to common awareness. People will simply use them unconsciously to accomplish everyday tasks... There are no systems that do well with the diversity of inputs to be found in an embodied virtuality.
" Weiser<weiser91>

Within this vision, input must move beyond the screen, supporting a wide range of embodied forms of computing. We'll begin by focusing on input techniques that rely on hands, just as pointing and text-entry largely have: physically touching a surface, using a pen-shaped object to touch a surface, and moving the hand or wrist to convey a gesture. Throughout, we will discuss how each of these forms of interaction imposes unique gulfs of execution and evaluation. 

|touchscreen.png|A diagram of a finger touching a touchscreen surface.|A 5-wire resistive touchscreen for sensing position.|Credit: Mercury13 CC BY-SA 3.0|

# Touch

Perhaps the most ubiquitous and familiar form of hand-based input is using our fingers to touchscreens. The first touchscreens originated in the mid-1960's. They worked similarly to modern touchscreens, just with less fidelity. The earliest screens consisted of an insulator panel with a resistive coating. When a conductive surface such as a finger made contact, it closed a circuit, flipping a binary input from off to on. It didn't read position, pressure, or other features of a touch, just that the surface was being touched. Resistive touchscreens came next, and rather than using capacitance to close a circuit, it relied on pressure to measure voltage flow between X wires and Y wires, allowing a position to be read. In the 1980's, HCI researcher [Bill Buxton|https://www.billbuxton.com]{Fun fact: Bill was my "academic grandfather", meaning that he was my advisor's advisor.} invented the first multi-touch screen while at the University of Toronto, placing a camera behind a frosted glass panel, and using machine vision to detect different black spots from finger occlusion. This led to several other advancements in sensing technologies that did not require a camera, and in the 1990's, multi-touchscreens launched on consumer devices, including handheld devices like the [Apple Newton|https://en.wikipedia.org/wiki/Apple_Newton] and the [Palm Pilot|https://en.wikipedia.org/wiki/Palm_(PDA)]. The 2000's brought even more innovation in sensing technology, eventually making multi-touchscreens small enough to embed in the smartphones we use today. (See [ArsTechnica's feature on the history of multi-touch|https://arstechnica.com/gadgets/2013/04/from-touch-displays-to-the-surface-a-brief-history-of-touchscreen-technology/] for more history).

As you are probably already aware, touchscreens impose a wide range of gulfs of execution and evaluation on users. On first use, for example, it is difficult to know if a surface is touchable. One will often see children who are used to everything being a touchscreen attempt to touch non-touchscreens, confused that the screen isn't providing any feedback. Then, of course, touchscreens often operate via complex multi-fingered gestures. These have to be somehow taught to users, and successfully learned, before someone can successfully operate a touch interface. This learning requires careful feedback to address gulfs of evaluation, especially if a gesture isn't accurately performed. Most operating systems rely on the fact that people will learn how to operate touchscreens from other people, such as through a tutorial at a store. 

|https://www.youtube.com/embed/lNCPY4gBam4|Sphere: Multi-Touch Interactions on a Spherical Display|Spherical multitouch|Microsoft Research<benko08>|

While touchscreens might seem ubiquitous and well understood, HCI research has been pushing its limits even further. Some of this work has invented new types of touch sensors. For example, researchers have worked on materials that allow touch surfaces to be cut into arbitrary shapes and sizes other than rectangles.<olberding13> Some have worked on touch surfaces made of foil and magnets that can sense bending and pressure<rendl12>, or thin, stretchable, transparent surfaces that can detect force, pinching, and dragging.<sugiura12> Others have made 3-dimensional spherical touch surfaces<benko08> and explored using _any_ surface as a touchscreen using depth-sensing cameras and projectors.<harrison11a>

Other researchers have explored ways of more precise sensing of _how_ a touchscreen is touched. Some have added speakers to detect how something was grasped or touched<ono13>, or leveraged variations in the resonance of people's finger anatomy to recognize different fingers and parts of different fingers<harrison11b>, or used the resonance of surfaces to detect and classify different types of surface scratching<harrison08>, including through fabric.<saponas11> Depth cameras can also be used to detect the posture and handedness of touch.<murugappan12> All of these represent new channels of input that go beyond position, allowing for new, richer, more powerful interfaces. 

|https://www.youtube.com/embed/EwRjb4fNWAI|DiamondTouch|Multi-user multi-touch|MERL<dietz01>|

Commercial touchscreens still focus on single-user interface, only allowing one person at a time to touch a screen. Research, however, has explored many ways to differentiate between multiple people using a single touch-screen. One approach is to have users sit on a surface that determines their identity, differentiating touch input.<dietz01> Another approach uses wearables to differentiate users.<webb16> Less obtrusive techniques have successfully used variation in user bone-density, muscle mass, and footwear<harrison12>, or fingerprint detection embedded in a display.<holz13>

While these inventions have richly explored many possible new forms of interaction, there is so far very little appetite for touchscreen innovation in industry. Apple's force-sensitive touchscreen interactions (called "3D touch") is one example of an innovation that made it to market, but there are some indicators that Apple will abandon it after just a few short years of users not being able to discover it (a classic gulf of execution). 

|pencil.jpg|An Apple Pencil input device.|The Apple Pencil.|Brett Jordan CC BY 2.0|

# Pens

In addition to fingers, many researchers have explored the unique benefits of pen-based interactions to support handwriting, sketching, diagramming, or other touch-based interactions. These leverage the skill of grasping a pen or pencil that many are familiar with from manual writing. Pens are similar to using a mouse as a pointing device in that they both involve pointing, but pens are critically different in that they involve direct physical contact with targets of interest. This directness requires different sensing technologies, provides more degrees of freedom for movement and input, and rel more fully on the hand's complex musculature. 

Some of these pen-based interactions are simply replacements for fingers. For example, the Palm Pilot, popular in the 1990's, required the use of a stylus for it's resistive touch-screen, but the pens themselves were plastic. They merely served to prevent fatigue from applying pressure to the screen with a finger and to increase the precision of touch during handwriting or interface interactions. 

|https://www.youtube.com/embed/OEQFq44wlKA|Palm Pilot Professional PDA Organizer|A demonstration of an early Palm Pilot device, which relied on pen-based input.|OSReviews|

However, pens impose their own unique gulfs of execution and evaluation. For example, many pens are not active until a device is set to a mode to receive pen input. The Apple Pencil, for example, only works in particular modes and interfaces, and so it is up to a person to experiment with an interface to discover whether it is pencil compatible. Pens themselves can also have buttons and switches that control modes in software, which require people to learn what the modes control and what effect they have on input and interaction. Pens also sometimes fail to play well with the need to enter text, as typing is faster than tapping one character at a time with a pen. One consequence of these gulfs of execution and efficiency issues is that pens are often used for specific applications such as drawing or sketching, where someone can focus on learning the pen's capabilities and is unlikely to be entering much text. 

Researchers have explored new types of pen interactions that attempt to break beyond these niche applications. For example, some techniques explore a user using touch input with a non-dominant hand, and pen with a dominant hand<hinckley10,hamilton12>, affording new forms of bi-manual input that have higher throughput than just one hand. Others have investigated ways of using the size of a pen's head to add another channel of input<bi08>, or even using a physical pen barrel, but with a virtual head, allowing for increased efficiency through software-based precision and customization.<lee12>

Other pen-based innovations are purely software based. For example, some interactions improve handwriting recognition by allowing users to correct recognition errors while writing<shilman06>, attempting to make the interplay between pen input and text input more seamless. Others have explored techniques for interacting with large displays for sketching and brainstorming activities.<guimbretière01> Researchers have developed interactions for particular sketching media, such as algorithms that allow painting that respects edges within images<olsen08> and diagramming tools that follow the paradigms of pencil-based architectural sketching.<zeleznik08> More recent techniques use software-based motion tracking and a camera to support six degree-of-freedom sub-millimeter accuracy.<wu17>

|gestures.jpg|Sixteen unistroke gestures.|Sixteen unistroke gestures|Wobbrock et al.<wobbrock07>|

# Gestures

Whereas touch and pens involve traditional [pointing|pointing], gesture-based interactions involve recognizing patterns in hand movement. Some gestures still recognize a gesture from a time-series of points in a 2-dimensional plane, such as the type of multi-touch gestures such as pinching and dragging on a touchscreen, or symbol recognition in handwriting or text entry. This type of gesture recognition can be done with a relatively simple recognition algorithm.<wobbrock07>

Other gestures rely on 3-dimensional input about the position of fingers and hands in space. Some recognition algorithms seek to recognize discrete hand positions, such as when the user brings their thumb and forefinger together (a pinch gesture).<wilson06> Researchers have developed tools to make it easier for developers to build applications that respond to in-air hand gestures.<krupka17> Other techniques try to model hand gestures using alternative techniques such as Electrical Impedance Tomography (EIT)<zhang15>, radio frequencies<wang16>, the electromagnetic field pulsed by GSM in a phone<zhao14>, or full machine vision of in-air hand gestures.<colaço13,song14> Some researchers have leveraged wearables to simplify recognition and increase recognition accuracy. These have included sensors mounted on fingers<gupta16b>, movement of a smartwatch through wrist rotations<zhang16>, while walking<gong16>, or while being tapped or scratched.<laput16>

|https://www.youtube.com/embed/Poi0MeASmuY|ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers|A demonstration of the ViBand research prototype, which uses bio-acoustic sensing via commericial smartwatch accelerometers|Laput et al.<laput16>|

While all of these inventions are exciting in their potential, gestures have significant gulfs of execution and evaluation. How does someone learn the gestures? How do we create tutorials that give feedback on correct gesture "posture"? When someone performs a gesture incorrectly, how can someone undo it if it had an unintended effect? What if the undo gesture is performed incorrectly? These questions ultimately arise from the unreliability of gesture classification.

# Hand Tracking

|color_glove.jpg|Color glove used for hand tracking|A glove used to faciliate hand tracking with cameras.|Wang et al.<wang09>|

Gesture-based systems look at patterns in hand motion to recognize a set of discrete poses or gestures. This is often appropriate when the user wants to trigger some action, but it does not offer the fidelity to support continuous hand-based actions, such as physical manipulation tasks in 3D space that require continuous tracking of hand and finger positions over time. Hand tracking systems are better suited for these tasks because they treat the hand as a continuous input device, rather than a gesture as a discrete event, estimating in real-time the hand’s position and orientation. 

Most hand tracking systems use cameras and computer vision techniques to track the hand in space. These systems often rely on an approximate model of the hand skeleton, including bones and joints, and solve for the joint angles and hand pose that best fits the observed data. Researchers have used gloves with unique color patterns, shown above, to make the hand easier to identify and to simplify the process of pose estimation.<wang09>

Since then, researchers have developed and refined techniques using depth cameras like the Kinect for tracking the hand without the use of markers or gloves.<wang11,oberweger15,taylor16,mueller17> Commercial devices, such as the [Leap Motion|https://www.leapmotion.com] have been developed that bring hand tracking to computers or virtual reality devices. These tracking systems have been used for interaction on large displays<liu15> and haptic devices.<long14>

|https://www.youtube.com/embed/QTz1zQAnMcU|Efficient and Precise Interactive Hand Tracking|Hand tracking system from Microsoft Research. With precise tracking, hands can be used to manipulate virtual widgets.|Taylor et al.<taylor16>|

For head-mounted virtual and augmented reality systems, a common way to track the hands is through the use of positionally tracked controllers. Systems such as the Oculus Rift or HTC Vive, use cameras and infrared LEDs to track both the position and orientation of the controllers. 

Like gesture interactions, the potential for classification error in hand tracking interactions can impose significant gulfs of execution and evaluation. However, because the applications of hand tracking often involve manipulation of 3D objects rather than invoking commands, the severity of these gulfs may be lower in practice. This is because object manipulation is essentially the same as direct manipulation: it's easy to see what effect the hand tracking is having and correct it if the tracking is failing. 

---

While there has been incredible innovation in hand-based input, there are still many open challenges. They can be hard to learn for new users, requiring careful attention to tutorials and training. And, because of the potential for recognition error, interfaces need some way of helping people correct errors, undo commands, and try again. Moreover, because all of these input techniques use hands, few are accessible to people with severe motor impairments in their hands, people lacking hands altogether, or if the interfaces use visual feedback to bridge gulfs of evaluation, people lacking sight. In the next chapter, we will discuss techniques that rely on other parts of a human body for input, and therefore can be more accessible to people with motor impairments.
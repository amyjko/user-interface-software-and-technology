Thus far, we have focused on universal issues in user interface design and implementation. This chapter will be the first in which we discuss specific paradigms of interaction and the specific theory that underlies them. Our first topic will be *pointing*.

# What is pointing?
Fingers can be pretty handy. (Sorry for the pun, I couldn't resist!). When able, we use them to grasp nearly everything. We use them to communicate through signs and gesture. And at a surprisingly frequent rate each day, we use them to _point_, in order to indicate, as precisely as we can, the identity of something (that person, that table, this picture, those flowers). As a nearly universal form of non-verbal communication, it's not surprising then that pointing has been such a powerful paradigm of interaction with user interfaces (that icon, that button, this text, etc.).

Pointing is not strictly related to interface design. In fact, back in the early 1950s, Paul M. Fitts was very interested in modeling human performance of pointing. He began developing predictive models about pointing in order to help design dashboards, cockpits, and other industrial designs for manufacturing. His focus was on "aimed" movements, in which a person has a *target* they want to indicate and must move their pointing device (originally, their hand) to indicate that target. This kind of pointing is an example of a "closed loop" motion, in which a system (e.g., the human) can react to its evolving state (e.g., here the human's finger is in space relative to its target). A person's reaction is the continuous correction of their trajectory as they move toward a target. Fitts began measuring this closed loop movement toward targets, searching for a pattern that fit the data, and eventually found this law, which we call *Fitts' Law*:

|fitts.jpg|A diagram of Fitt's law, showing A, the distance to target, and W, the size of the target, and the formula MT = a + b * log(A/W + 1)|Fitts' Law.|Jacob Wobbrock|

 Let's deconstruct this equation. The formula computes the time to reach a target (*MT* refers to "motion time"). The *A* in the figure is how far one must move to reach the target (e.g. how far your finger has to move from where it is to reach a target on your phone's touch screen. The *W* is the size (or width) of the target (e.g., the physical length of an icon on your smartphone's user interface). The units on these two measures don't really matter as long as they're the same, because the formula above computes the ratio between the two, canceling the units out. The *a* and *b* coefficients are user- and device-specific constants, where *a* is some fixed constant minimum time to move and *b* is a measure of how efficiently movement occurs. Each of these coefficients differ by user and device.

So what does the formula _mean_? Let's play with the algebra. When *A* (distance to target) goes up, time to reach the target increases. That makes sense, right? If you're using a touch screen and your finger is far from the target, it will take longer to reach the target. What about *W* (size of target)? When that goes up, the movement time goes _down_. That also makes sense, because easier targets (e.g., bigger icons) are easier to reach. The implications of this are quite simple: the closer and bigger the target, the faster it will be to point to.
		
# What does pointing have to do with user interfaces?
But there are some other interesting implications for user interface design in the algebraic extremes. For example, what if the target size is _infinite_? An example of this is the command menu in Apple Mac OS applications, which is always placed at the very top of a screen. The target in this case is the top of the screen, which is effectively infinite in size, because no matter how far past the top of the screen you point with a mouse, the operating system always constrains the mouse position to be within the screen boundaries. This makes the top of the screen (and really any side of the screen) a target of infinite size. A similar example is the Windows Start button, anchored to the corner of the screen. And according to Fitts' Law, these infinite target sizes means effectively zero movement time. That's why it's so quick to reach the menu on Mac OS and the Start button on Windows: you can't miss.

|infinite-target.png|A screenshot of the Mac OS application menu at the top of a screen.|Apple's menu bar has infinite size, making it quick to reach|Amy J. Ko|

What's surprising about Fitts' Law is that, as far as we know, it applies to any kind of pointing: using a mouse, using a touch screen, using a trackball, using a trackpad, or reaching for an object in the physical world. That's conceptually powerful because it means that you can use the idea of large targets and short distance to design interfaces that are efficient to use. As a mathematical model used for prediction, it's less powerful: to really predict exactly how long a motion will take, you'd need to estimate a distribution of those *a* and *b* coefficients for a large group of possible users and devices. Researchers carefully studying motion might use it to do precise modeling, but for designers, the concepts it uses are more important.

Now that we have Fitts' law as a conceptual foundation for design, let's consider some concrete design ideas for pointing in interfaces. There are so many kinds: mice, styluses, touch screens, touch pads, joysticks, trackballs, and many other devices. Some of these devices are *direct pointing* devices (e.g., touch screens), in which input and output occur in the same physical place (e.g., a screen or some other surface). In contrast, *indirect pointing* devices (e.g., a mouse, a touchpad, a trackball) provide input in a different physical place from where output occurs (e.g., input on a device, output on a non-interactive screen). Each has their limitations: direct pointing can result in *occlusion* where a person's hand obscures output, and indirect pointing requires a person to attend to two different places.

There's also a difference between *absolute* and *relative* pointing. Absolute pointing includes input devices where the physical coordinate space of input is mapped directly onto the coordinate space in the interface. This is how touch screens work (bottom left of the touch screen is bottom left of the interface). In contrast, relative pointing maps _changes_ in a person's pointing to changes in the interface's coordinate space. For example, moving a mouse left an inch is translated to moving a virtual cursor some number of pixels left. That’s true regardless of where the mouse is in physical space. Relative pointing allows for variable *gain*, meaning that mouse cursors can move faster or slower depending on a user’s preferences. In contrast, absolute pointing cannot have variable gain, since the speed of interface motion is tied to the speed of a user’s physical motion.
 	
# How can we make pointing better?

When you think about these two dimensions from a Fitts' law perspective, making input more efficient is partly about inventing input devices that minimize the *a* and *b* coefficients. For example, researchers have invented new kinds of mice that have multi-touch on them, allowing users to more easily provide input during pointing movements<villar09>. Other research has explored taking normal pointing devices and preserving the physical measurements of input rather than mapping them to integer number spaces, enabling	sub-pixel precision in mouse movement for tasks like minute-level precision in calendar event creation, pixel-level image cropping, and precise video frame selection<roussel12>. Others have invented new types of pointing devices altogether, such as the LightRing (see in the video below), which involves sensing of infrared proximity between a finger and a surface to point<kienzle14>. Some devices have been invented for 3D pointing using magnetic field sensing<chen13>. Other techniques have tried to detect individual fingers on touchscreens, enabling multiple independent streams of pointing input<gupta16>. All of these device manipulations seek to decrease pointing time by increasing how much input a person can provide, while decreasing effort.

|https://www.youtube.com/embed/3_GbVIo6iq0|LightRing: Always-Available 2D Input on Any Surface|LightRing|Microsoft Research|

Other innovations focus on software, and aim to increase target size or reduce travel distance. Many of these ideas are *target-agnostic* approaches that have no awareness about what a user might be pointing _to_. Some target-agnostic techniques include things like mouse pointer acceleration, which is a feature that makes the pointer move faster if it determines the user is trying to travel a large distance<casiez08>. This technique is target-agnostic because it doesn't know where the mouse is moving to, it just knows that the pointer is moving fast. Another example is the Angle Mouse, which analyzes the angles of movement trajectory, reducing gain when a user is trying to "turn," effectively making the target larger by slowing down the mouse, making it easier to reach<wobbrock09>. This technique is target-agnostic because it doesn't know where the user is turning to, just that the pointer is turning. These ideas are powerful because the operating system does not need any awareness of the things a user is trying to point to in a user interface, making it easier those interfaces easier to implement.

Other pointing innovations are *target-aware*, in that the technique needs to know the location of things that a user might be pointing to so that it can adapt based on target locations. For example, area cursors are the idea of having a mouse cursor represent an entire two-dimensional space rather than a single point, reducing the distance to targets. These have been applied to help users with motor impairments<findlater10>. The Bubble Cursor is an area cursor that dynamically resizes a cursor's activation area based on proximity to a target, growing it to the maximum possible size based on the surrounding targets<grossman05>. Cursors like this make it much easier for people with motor impairments (e.g., Parkinson’s disease), to click on targets with a mouse, even when their hands "tremor", making unexpected, unwanted movements. You can see one improvement to the Bubble Cursor, the Bubble Lens, in the video below, showing how the Lens magnifies content in target-dense spaces<mott14>, making it even easier for people who lack the ability to finely coordinate their motor movements to click on targets.

|https://www.youtube.com/embed/9b8QqLungzc|Beating the Bubble: Using Kinematic Triggering in the Bubble Lens for Acquiring Small, Dense Targets|The Bubble Lens, a target-aware pointing technique|Mott & Wobbrock<mott14>|

Snapping is another target-aware technique commonly found in graphic design tools, in which a mouse cursor is constrained to a location based on nearby targets, reducing target distance. Researchers have made snapping work across multiple dimensions simultaneously<felice16> and have even applied it to things like scrolling, snapping to interesting content areas<kim14>. Another clever idea is the notion of _crossing_, which, instead of pointing and clicking, involves moving across a "goal line"<apitz04>. Shown in the image below, crossing can allow for fluid movements through commands and controls without ever clicking.

|CrossY.png|A screenshot of CrossY, showing the motion of a mouse cursor through three controls, all selected without clicking|CrossY uses a goal crossing interaction to avoid the need to click to select targets.|Apitz et al.<apitz04>|

While target-aware techniques can be even more efficient than target-agnostic ones, making an operating system aware of targets can be hard, because user interfaces can be architected to process pointing input in such a variety of ways. Some research has focused on overcoming this challenge. For example, one approach reverse-engineered the widgets on a screen by analyzing the rendered pixels to identify targets, then applied targeted-aware pointing techniques like the bubble cursor<dixon12>.  Another technique used a data-driven technique, monitoring where in a window users typically point, and then assuming targets are there, applying target-aware pointing techniques<hurst07b>. Some have gone as far as using brain-sensing techniques to detect when someone is in a period of difficult multitasking, then during that period, increasing the size of high priority targets, while decreasing the size of low priority targets<afergan14>.

You might be wondering: all this work for faster pointing? Fitts' law and its focus on speed _is_ a very narrow way to think about the experience of pointing to computers. And yet, it is such a fundamental and frequent part of how we interact with computers; making pointing fast and smooth is key to allowing a person to focus on their task and not on the low-level act of pointing. This is particularly true of people with motor impairments, which interfere with their ability to precisely point: every incremental improvement in one's ability to precisely point to a target might amount to hundreds or thousands of easier interactions a day, especially for people who depend on computers to communicate and connect with the world.
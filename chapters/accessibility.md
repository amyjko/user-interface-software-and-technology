Thus far, most of our discussion has focused on what user interfaces _are_. I [described them theoretically|theory] as _a mapping from the sensory, cognitive, and social human world to these collections of functions exposed by a computer program_. While that's true, most of the mappings we've discussed have been input via our fingers and output to our visual perceptual systems. Most user interfaces have largely ignored other sources of human action and perception. We can speak. We can control over 600 different muscles. We can convey hundreds of types of non-verbal information through our gaze, posture, and orientation. We can see, hear, taste, smell, sense pressure, sense temperature, sense balance, sense our position in space, and feel pain, among dozens of other senses. This vast range of human abilities is largely unused in interface design. 

This bias is understandable. Our fingers are incredibly agile, precise, and high bandwidth sources of action. Our visual perception is similarly rich, and one of the dominant ways that we engage with the physical world. Optimizing interfaces for these modalities is smart because it optimizes our ability to use interfaces. 

However, this bias is also unreasonable because not everyone can see, use their fingers precisely, or read text. Designing interfaces that can _only_ be used if one has these abilities means that vast numbers of people simply can't use interfaces<ladner12>. And this is no small population: according to a [2004 World Bank survey|http://www.worldbank.org/en/topic/disability] (World Bank is an international financial institution that seeks to reduce poverty), one billion people (15% of humanity) have some form of disability that impacts their daily activities. That includes people who are blind, low-vision, color blind, deaf, hard of hearing, unable to speak, or possess speech impediments, inability to walk or use limbs, or have some form of cognitive disorder such as dyslexia, dysgraphia, dyscalculia, memory loss, or learning disabilities. This might describe you. This might describe someone you know. Chances are, you will be disabled in one or more of these ways someday as you age. And that means you'll struggle or be unable to use the graphical user interfaces you've worked so hard to learn. And if you know no one that struggles with interfaces, it may be because they are stigmatized by their difficulties, not sharing their struggles and avoiding access technologies because they signal disability<shinohara11>. Or worse yet, they are not even in your social world, because their inability to use interfaces has led to their systematic exclusion. And if justice and inclusion are not justification enough, remember that every business that ignores accessibility is ignoring 15% of people in the world as potential customers. 

Of course, abilities vary, and this variation has different impacts on people's ability to use interfaces. One of the most common forms of disability is blindness and low-vision. But even within these categories, there is diversity. Some people are completely blind, some have some sight but need magnification. Some people have color blindness, which can be minimally impactful, unless an interface relies heavily on colors that a person cannot distinguish. I am near-sighted, but still need glasses to interact with user interfaces close to my face. When I do not have my glasses, I have to rely on magnification to see visual aspects of user interface. And while the largest group of people with disabilities are those with vision issues, the long tail of other disabilities around speech, hearing, and motor ability, when combined, is just as large.

Of course, most interfaces assume that none of this variation exists. And ironically, it's partly because the user interface toolkits we described in [the architecture chapter|architecture] embed this assumption deep in their architecture. Toolkits make it so easy to design graphical user interfaces that these are the only kind of interfaces designers make. This results in most interfaces being difficult or sometimes impossible for vast populations to use, which really makes no business sense<horton15>. We use the word *accessibility* to refer to the extent to which an interface can be used by people regardless of their abilities. We describe user interfaces as *universal* if they can be used by people regardless of what combination of abilities they have<story98>. Achieving universal design means achieving principles like ensuring that use is equitable, flexible, simple, that information be perceived and accessed by all, that error rates are low for everyone, and that physical effort is minimal. These principles can be vague, but that is because abilities are diverse: no one statement can summarize all of the abilities one must account for in user interface design. 

Whereas universal interfaces work for everyone as is, *access technologies* are alternative user interfaces that attempt to make an existing user interface more universal. Access technologies include things like:

* *Screen readers* convert text on a graphical user interface to synthesized speech so that people who are blind or unable to read can interact with the interface.
* *Captions* annotate the speech and action in video as text, allowing people who are deaf or hard of hearing to consume the audio content of video.
* *Braille*, as shown in the image at the top of this chapter, is a tactile encoding of words for people who are visually impaired. 

Consider, for example, this demonstration of screen readers and a braille display:

|https://www.youtube.com/embed/7Rs3YpsnfoI|Using a Screen Reader|Screen readers and braille output|The DOIT Center|

Fundamentally, universal user interface designs are ones that can be operated via _any input and output modality_. If user interfaces are really just ways of accessing functions defined in a computer program, there's really nothing about a user interface that requires it to be visual or operated with fingers. Take, for example, an ATM machine. Why is it structured as a large screen with several buttons? A speech interface could expose identical banking functionality through speech and hearing. Or, imagine an interface in which a camera just looks at someone's wallet and their face and figures out what they need: more cash, to deposit a check, to check their balance. The input and output modalities an interface uses to expose functionality is really arbitrary: using fingers and eyes is just easier for most people in most situations. 

|modalities.jpg|A diagram showing fingers, speech, and a mouse as input to a function, with screens, sound, and tactile feedback as output.|Universal interfaces allow functions to be invoked through any input modality, and allow function's output and side effects to be perceived through any output modality.|Amy J. Ko|

The challenge for user interface designers then is to not only design the functionality a user interface exposes, but also design a myriad of ways of accessing that functionality through any modality. Unfortunately, conceiving of ways to use all of our senses and abilities is not easy. It took us more than 20 years to invent graphical user interfaces optimized for sight and hands. It's taken another 20 years to optimize touch-based interactions. It's not surprising that it's taking us just as long or longer to invent seamless interfaces for speech, gesture, and other uses of our muscles, and efficient ways of perceiving user interface output through hearing, feeling, and other senses. 

These inventions, however, are numerous and span the full spectrum of modalities. For instance, access technologies like screen readers have been around since shortly after [Section 508|https://www.section508.gov/content/learn] of the Rehabilitation Act of 1973, and have converted digital text into synthesized speech. This has made it possible for people who are blind or have low-vision to interact with graphical user interfaces. But now, interfaces go well beyond desktop GUIs. For example, just before the ubiquity of touch screens, the SlideRule system showed how to make touch screens accessible to blind users by reading labels of UI throughout multi-touch<kane08>. This research impacted the design of Apple's VoiceOver functionality in iOS, which influenced Microsoft and Google to add multi-touch screen reading to Windows and Android. These advances in access technologies, especially when built in at the operating system level, have greatly increased the diversity of people who can access computing.

|https://www.youtube.com/embed/Wy0656j2eig|VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World|VizLens|Guo et al.<guo16>|

For decades, screen readers have only worked on computers, but recent innovations like _VizLens_ (above) have combined machine vision and crowdsourcing to support arbitrary interfaces in the world, such as microwaves, refrigerators, ATMs, and other appliances<guo16>. Innovations like this allow users to capture the interface with a camera, then get interactive guidance of the layout and labels of the interface. Solutions like this essentially provide screen reading for anything in the world, converting visual interfaces into auditory ones. 

With the rapid rise in popularity of the web, web accessibility has also been a popular topic of research. Problems abound, but one of the most notable is the inaccessibility of images. Images on the web often come without `alt` tags that describe the image for people unable to see. User interface controls often lack labels for screen readers to read. Some work shows that some of the the information needs in these descriptions are universal—people describing needing descriptions of people and objects in images—but other needs are highly specific to a context, such as subjective descriptions of people on dating websites<stangl20>. Other works hows that emoji are a particularly inaccessible form of text content, lacking not only descriptors, but also disruptive features like repeated emoji sequences that are read by screen readers redundantly<tigwell20>.  Researchers have only just begun inventing ways of inferring these descriptions and labels by mining the surrounding context of a page for a reasonable description<islam10> or by using machine vision to identify and track personal objects at someone's home while giving verbal and auditory feedback about location<ahmetovic20>. People with low-vision often just need magnification of content. While web browsers allow people to increase text size, this often breaks the layout of pages, and so researchers have invented ways of automatically resizing images and text to a size that doesn't break layout, improving readability automatically<bigham14>. One way to view these innovations is as bandages to accessibility flaws at the architectural level. For example, why is it valid to leave an “alt” tag empty in HTML? Why do HTML layout algorithms allow text to overlap? Little work has considered how to design architectures, user interface tools, and toolkits that prevent accessibility problems. And the patches that have been invented are still far from meeting even basic needs of people with visual disabilities.

For people who are deaf or hard of hearing, videos, dialog, or other audio output interfaces are a major accessibility barrier to using computers or engaging in computer-mediated communication. Researchers have invented systems like _Legion:Scribe_, which harness crowd workers to provide real-time captioning of arbitrary audio streams with only a few seconds of latency<lasecki12>. People who are deaf may also use sign language, but computer-mediated communication like video chat often has insufficient frame rates and resolution to read signs. Researchers have invented video compression algorithms that optimize for detail in hands at the expense of other visual information<cherniavsky09>. Not only did this technique make it more feasible for people to sign via low-frame-rate video chat, but the compression algorithm also increased battery life by 47% by reducing bandwidth requirements. 

|edgewrite.png|The _EdgeWrite_ gesture set, which includes an alphabet that involves tracing along edges of a square. The gestures resemble Roman characters.|The EdgeWrite gesture set.|Wobbrock et al.<wobbrock03>|

For people who have motor impairments, such as [motor tremors|https://www.youtube.com/watch?v=-Y3kex_8UoY], fine control over mouse, keyboards, or multi-touch interfaces can be quite challenging, especially for tasks like text-entry, which require very precise movements. Researchers have explored several ways to make interfaces more accessible for people without fine motor control. _EdgeWrite_, for example, is a gesture set (shown above) that only requires tracing the edges and diagonals of a square<wobbrock03>. This stabilizes motion even in the presence of tremors, significantly increasing text entry speed and correctness. To make mouse and touch-based interfaces more accessible, systems like _SUPPLE_ aimed to model users' motor control abilities, and used that model to generate a custom interface layout that made user interface controls easier to click, while preventing accidental clicks<gajos07>. _SmartTouch_ used a similar approach, modeling how people with a wide range of motor impairments touched touch-screens, and adapting the algorithms that inferred the intended touch point based on these models<mott16>. Both _SUPPLE_ and _SmartTouch_ are examples of *ability-based design*, in which the user interface models the user and adapts itself to the users' abilities<wobbrock11>.

|https://www.youtube.com/embed/lju6IIteg9Q|Prefab: What if We Could Modify Any Interface?|Prefab<dixon10>|Dixon et al.|

Sight, hearing, and motor abilities have been the major focus of innovation, but an increasing body of work also considers neurodiversity. For example, many adults have intellectual disabilities, such as an autism spectrum disorder, down syndrome, fetal alcohol syndrome, and others, all of which can reduce ability to understand new or complex information. As we have discussed, interfaces can be a major source of such information complexity. This has led to interface innovations that facilitate a range of aids, including images and videos for conveying information, iconographic, recognition-based speech generation for communication, carefully designed digital surveys to gather information in health contexts, and memory-aids to facilitate recall. Some work has found that while these interfaces can facilitate communication, they are often not independent solutions, and require exceptional customization to be useful<gibson20>.

Whereas all of the innovations above aimed to make particular types of information accessible to people with particular abilities, some techniques target accessibility problems at the level of software architecture. For example, accessibility frameworks and features like Apple's [_VoiceOver_|https://www.apple.com/accessibility/mac/vision/] in iOS are system-wide: when a developer uses Apple's standard user interface toolkits to build a UI, the UI is automatically compatible with _VoiceOver_, and therefore automatically screen-readable. Because it's often difficult to convince developers of operating systems and user interfaces to make their software accessible, researchers have also explored ways of modifying interfaces automatically. For example, _Prefab_ (above) is an approach that recognizes the user interface controls based on how they are rendered on-screen, which allows it to build a model of the UI's layout<dixon10>. This allows _Prefab_ to intercept mouse and touch input and leverage the wide range of accessibility optimizations for pointing from other researchers to make the target interface easier to operate. While _Prefab_ focuses on user interface controls, the Genie system focuses on the underlying functions and commands of a user interface<swearngin17>. It reverse engineers a model of all of the commands in an interface, and then can automatically repackage those commands in alternative user interfaces that are more accessible. 

While all of the ideas above can make interfaces more universal, they can also have other unintended benefits for people without disabilities. For example, it turns out screen readers are great for people with ADHD, who may have an easier time attending to speech than text. Making web content more readable for people with low-vision also makes it easier for people with situational impairments, such as dilated pupils after a eye doctor appointment. Captions in videos aren't just good for people who are deaf and hard of hearing; they're also good for watching video in quiet spaces. Investing in these accessibility innovations then isn't just about impacting that 15% of people with disabilities, but also the rest of humanity. 


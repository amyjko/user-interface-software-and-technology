<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Latest compiled and minified CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

		<!-- Optional theme -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

		<link rel="stylesheet" href="style.css" />

		<title>Interface ethics</title>

	</head>
	<body>

		<p><a href="index.html">Back to table of contents</a></p>

		<img src="images/ring.jpg" class="img-responsive" alt="A photograph of a Ring Doorbell mounted on a brick wall." />
		<small>A Ring video doorbell, spying on the neighbors.</small>

		<h1>Interface ethics</h1>
		<div class="lead">Amy J. Ko</div>
    
		<p>
			The ethics of computing have never been more visible.
			CEOs of major tech companies are being invited to <a href="https://www.nytimes.com/2018/04/10/us/politics/mark-zuckerberg-testimony.html">testify</a> in front of governments about their use of data.
			Driverless cars are raising questions about whether machines should be deciding <a href="https://www.nytimes.com/2016/06/24/technology/should-your-driverless-car-hit-a-pedestrian-to-save-your-life.html">who does and doesn't die</a>.
			Judges are beginning to adopt machine learning to <a href="https://www.nytimes.com/2017/05/01/us/politics/sent-to-prison-by-a-software-programs-secret-algorithms.html">predict recidivism</a>, rather than using their own judgement.
		</p>
		
		<p>
			These and other applications of computing provoke numerous profound questions about design ethics.
		</p>
		
		<ul>
			<li>Who should we design for?</li>
			<li>How do we include the voices of all stakeholders in design?</li>
			<li>What responsibility to interaction designers have to create a sustainable, human future?</li>
		</ul>
		
		<p>
			There are many methods that try to answer these questions, ranging from inclusive design (Clarkson et al. 2013), universal design (Story 1998), participatory design (Schuler and Namioka 2017), and value-sensitive design (<a href="https://dl.acm.org/doi/pdf/10.1145/242485.242493">Friedman 1996</a>).
			And there are many practitioners grappling with the how to use these methods.
			Many are wondering whether these methods are enough to resolve the deep questions about the role of computing in society.
		<p>
			
		<p>
			But this book isn't about interaction design broadly, it's about interfaces, and the software and technology that make them possible.
			What specific role do interface technologies have in design ethics?
			And what role do interaction designers have in designing and leveraging interface technologies ethically?
		</p>
		
		<p>
			In this chapter, I argue that there are at least four ways that interfaces technologies are at the heart of interaction design ethics.
		</p>
		
		<h2>Interface technologies standardize, erasing diversity</h2>
		
		<p>
			One of the central roles of user interface software and technology is to <strong>standardize</strong> interaction.
			User interface toolkits lower the barrier to creating consistent interaction paradigms.
			User interface hardware, such as the sensor packages in phone, define what computers are capable of sensing.
			User interface conventions, built into software and hardware, regularize user experience, making new interfaces that follow convention easier to learn.
			These kinds of standardization aim for desirable ends of usability, learnability, and user efficiency.			
		</p>
		
		<p>
			Standardization is not ethically wrong in its own right.
			However, if not done carefully, the ubiquity of interface standards and conventions can exacerbate inequities in design choices.
			For example, the dominance of screen-based interaction with computers is fundamentally unjust toward people without sight or sufficient visual acuity to use screens.
			That standard is not a mere inconvenience to a population, but a blunt exclusion of people with disabilities from our computational worlds.
			The defaults and templates built into user interface developer tools, intended to streamline the prototyping of conventional interfaces, build in subtle assumptions about left-to-right languages.
			This default makes it easier to create interfaces that function well for western languages, and harder to create interfaces for eastern languages.
			As with screens, these defaults are a categorical exclusion of cultures around the world, framing top-to-bottom, right-to-left languages as exceptional and secondary.
			Interface conventions and standards can also embed cultural assumptions in them.
			For example, taking a photograph is a social affront in some cultures, but we place multiple camera sensors with prominence in our phones.
			Interface standardization is, in a way, colonialist, embedding the language, ability, and cultural assumptions of one culture (primarily Silicon Valley) onto another.
		</p>
		
		<p>
			One can question whether the colonialist mechanisms of interface standardization are unethical.
			But that is a debate that is more about the ethics of colonialism than interfaces.
			Do you believe it is the right of designers in Cupertino and Mountain View to embed their culture into interfaces reaching global ubiquity?
			And if not, are you willing to champion diversity in culture and ability at the expense of the learnability and interoperability that standards enable?
		</p>
		
		<p>
			As a designer, you decide.
		</p>
		
		<h2>Interface technologies reconfigure human experience</h2>
		
		<p>
			When I was a child, I had no computer, no smartphone, and no internet.
			When I was bored, I had a few simple options for how to entertain myself: read a (print) book, walk to a friend's house, sing a song, or play with toys with my brother.
			The nature of that experience was one of simplicity.
			When I didn't want to do any of those things, I would often choose to just sit and observe the world.
			I watched squirrels chase each other.
			I pondered the movement of the clouds.
			I listened to the rain's rhythms upon my roof.
			I looked to nature for stimulation, and it returned riches.				
		</p>
		
		<p>
			As I aged, and computing became embedded in my life, my choices changed.
			Interfaces were abundant.
			The computer in the den offered puzzles and games.
			My Super Nintendo offered social play with friends.
			My modem connected me to the nascent internet and it's rapidly expanding content.
			My palette of entertainment expanded, but in some ways narrowed: interacting with computers, or with friends through computers, was much more visceral, immediate, and engaging.
			It promised instant gratification, unlike nature, which often made me wait and watch.
			It could not compete with the bright lights of the screen, and the immediacy of a key press.
			My world shifted from interacting with people and nature, to interacting with computers.
		</p>
		
		<p>
			How much artificial is too much?
			Is there a right way to be a human?
			Is there a natural way to be a human?
			Does human-computer integration go too far (<a href="https://dl.acm.org/doi/pdf/10.1145/3001896">Farooq and Grudin 2016</a>)?
			Or is integration an inevitable embrace of our susceptibility to tight cycles of stimulus and response?
			Do we design for a world that centers human-computer interaction, or carefully situates it alongside the much broader and richer collection of other human experiences?
		</p>
		
		<p>
			As a designer, you decide.
		</p>
		
		<h2>Interface technologies amplify social choices</h2>
		
		<p>
			There is emerging agreement that computing does not <em>cause</em> social change, but <strong>amplifies</strong> it, in whatever way that change intends (<a href="https://doi.org/10.1145/1940761.1940772">Toyama 2011</a>).
			For example, social media has not caused political division, but it has amplified division that was already there.
			The Ring doorbell, pictured at the beginning of this chapter, has not caused ubiquitous surveillance, but it has amplified our ability to surveil our homes.
			Games have not caused social isolation, but they have amplified isolation, giving depressed adolescents a short term yet isolating salve for unwanted seclusion.
			In these ways, computing are like guns: they are tools that can amplify violence, far beyond what we can do with our hands, but they do not alone cause violence.
		</p>
		
		<p>
			If we accept the premise that human beings are ultimately responsible for both desirable and undesirable social change, and that interface technology are just the tools by which we achieve it, what implications does that have for interface design?
			If interfaces amplify, then that also amplifies the consequences of our choices of precisely what we amplify.
			For example, if we work on simplifying the control of drones, we must accept that we are helping hobbyists more easily surveil our neighborhoods and governments more easily drop bombs.
			If we work on simplifying the spread of information, we must accept that we are also simplifying the spread of misinformation and disinformation.
			If we design new interfaces for recreating or altering the appearance of actors in movies, we must accept that we are also enabling troubling <a href="https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them">deep fakes</a>.
		</p>
			
		<p>
			An amplification perspective ultimately forces us to question the ethics of what our interfaces enable.
			It forces us to think rigorously about both intended and unintended uses of our interfaces.
			It forces us to imagine not only the best case scenarios of use, but also the worst case scenarios.
			If forces us to take some responsibility as accomplices for the actions of others.
			But how much?
		</p>

		<p>
			As a designer, you decide.
		</p>

		<h2>Interface technologies waste</h2>

		<p>
			Before graphical user interfaces, there werenâ€™t that many computers. 
			Large companies had some mainframes that they stored away in large rooms. 
			Hobbyists built their own computers at a negligible scale. 
			The computer hardware they created and discarded had negligible impact on waste and sustainability.
		</p>
		<p>
			All of this changed with the graphical user interface.
			Suddenly, the ease with which one could operate a computer to create and share information, led to ubiquity.
			This ease of sharing information has led to a massive global demand for data, is leading data consumption to account for <a href="https://www.theguardian.com/environment/2017/dec/11/tsunami-of-data-could-consume-fifth-global-electricity-by-2025">20% of global CO2 emissions</a>, much of it in data centers controlled by just a few companies.
			Simultaneously, the promise of ever simpler and more useful interfaces leads to rapid upgrade cycles, leading e-waste to account for at least 44 million tons of waste.
			This makes computer hardware, and the interface accessories embedded in it, the fastest growing source of garbage on the planet. 
			Inside these millions of tons of garbage lie increasing proportion of the world's rare earth metals such as gold, platinum, cobalt, and copper, as well as numerous toxins.
		</p>

		<p>
			Would we have reached this level of CO2 output and waste without an immense effort to make interfaces learnable, efficient, useful, and desirable? 
			Likely not. 
			In this sense, innovations interface software and technology is responsible for creating the demand, and therefore responsible for the pollution and waste.
		</p>
		<p>
			Some companies are beginning to take responsibility for this.
			Apple started a <a href="https://www.apple.com/recycling/nationalservices/">recycling program</a> to help reclaim rare earth metals and prevent toxins from entering our water. 
			<a href="https://www.nytimes.com/2019/09/19/technology/amazon-carbon-neutral.html">Amazon pledged</a> to shift to sustainable energy sources for its warehouses and deliveries. 
			<a href="https://www.microsoft.com/en-us/corporate-responsibility/sustainability)">Microsoft pledged</a> to be not only carbon neutral, but carbon negative by 2035.
		</p>
			Are these efforts enough to offset the demand these companies generate for software, hardware, and data? 
			Or, perhaps consumers are responsible for their purchasing decisions. 
			Or, perhaps designers, who are the ones envisioning unsustainable, wasteful products and services are responsible for moving these companies? 
		</p>
		
		<p>
			As a designer, you decide.
		</p>
		
		<hr/>
		
		<p>
			How do you decide?
			The easiest way is, of course, to delegate.
			You can hope your manager, your VP, or your CEO has greater courage and insight than you.
			Alternatively, you can advocate, demanding change from within as many have done at major technology companies.
			If you organize well, make your message clear, and use your power in numbers, you can change what organizations do.
			And if that is unsuccessful, you can choose a different employer.
			Use the ample opportunity of the ever growing marketplace for interfaces to choose enterprises that care about justice, humanity, morality, and sustainability.
		</p>
		
		<p>
			To make these choices, you might need to clarify your values.
			You might need to build confidence in your skills and your ability to secure a new job, or start your own company.
			But long before you do, you will be faced with these choices, and the burden of responsibility.
			And as the designer, you will decide.
		</p>
    		
		<center><p class="lead"><a href="index.html">Back to table of contents</a></p></center>

		<h2>Further reading</h2>

			
		<p>
			Clarkson, P. J., Coleman, R., Keates, S., & Lebbon, C. (2013). Inclusive design: Design for the whole population. Springer Science & Business Media.
		</p>

		<p>
			Farooq, U., & Grudin, J. (2016). <a href="https://dl.acm.org/doi/pdf/10.1145/3001896">Human-computer integration</a>. interactions, 23(6), 26-32.
		</p>

		<p>
			Friedman, B. (1996). <a href="https://dl.acm.org/doi/pdf/10.1145/242485.242493">Value-sensitive design</a>. interactions, 3(6), 16-23.
		</p>
		
		<p>
			Schuler, D., & Namioka, A. (2017). Participatory design: Principles and practices. CRC Press.
		</p>
		
		<p>
			Story, M. F. (1998). Maximizing usability: the principles of universal design. Assistive technology, 10(1), 4-12.
		</p>
		
		<p>
			Toyama, K. (2011). <a href="https://doi.org/10.1145/1940761.1940772">Technology as amplifier in international development</a>. In Proceedings of the 2011 iConference.
		</p>


			
		<script type="text/javascript">
		
			var _gaq = _gaq || [];
			_gaq.push(['_setAccount', 'UA-10917999-1']);
			_gaq.push(['_trackPageview']);
			
			(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
			})();
		
		</script>

	</body>

</html>

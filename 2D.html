<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Latest compiled and minified CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

		<!-- Optional theme -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

		<link rel="stylesheet" href="style.css" />

		<title>2D Visual Output</title>

	</head>
	<body>

		<p><a href="index.html">Back to table of contents</a></p>

		<img src="images/pong.png" class="img-responsive" alt="A screenshot of the original Pong video game with two paddles, a ball, and scores." />
		<small>Pong, one of the earliest arcade video games.</small>

		<h1>2D Output</h1>

		<div class="lead">Andrew J. Ko</div>

		<p>
			It's easy to forget that computers didn't always have screens.
			The original format for computer output was actually printed, not rendered, and as with even modern printers, printing was slow.
			It wasn't until <a href="history.html">Ivan Sutherland integrated a CRT screen with a computer in Sketchpad</a> that screens enabled interactive, immediate feedback experiences.
		</p>

		<p>
			But screens alone were not enough.
			There was an entire set of new concepts that needed to be invented to make use of screens, ranging from graphics, typography, images, visualization, and animated versions of all of these media.
			And researchers continue to innovate in these spaces, including in screen technology itself.
			In this chapter, we'll review screen technology, and then discuss how these media were translated to computer screens and further enhanced.
		</p>

		<h2>Screens</h2>

		<p>
			To begin, let's consider screens themselves.
			Some of the earliest screen technology used something called a CRT (Cathode Ray Tube).
			This was a vacuum tube with an electron gun and a phosphorescent screen.
			The device moves the electron gun in a repetitive pattern called a "raster" scan across the two dimensions of the screen, causing the phosphorescent material to glow at whatever points it was active.
			To make color CRTs, three electron guns are used, one for red, green, and blue color.
			To determine what to draw on screen, computers stored in memory a long list of color values, and then hardware translated those color values at high frequency during the raster to determine when the electron guns were on and off.
			When this happens at a high frequency (generally 24 times a second or faster), we get the interactive screens we're used to today.
		</p>

		<p>
			<img src="images/crt.jpg" class="img-responsive" alt="A cathode ray tube emitter." />
			<center>A cathode ray tube emitter. Credit: MaxPixel.</center>
		</p>

		<p>
			The problem with CRTs was that they were <em>huge and heavy</em>, making them practical only for desktop use.
			Display technology evolved to solve these problems, with liquid crystal displays (LCDs) making the next leap.
			LCDs, which are still quite common in devices today, are big grids of red, green, and blue <em>liquid crystals</em>.
			Liquid crystals are a state of matter between liquid and solid with varying optical qualities.
			By placing this grid of liquid crystals on top of a big backlight, these crystals filter light in red, green, and blue at different intensities based on the current running through the liquid.
			These crystals are tiny, allowing for screens that were flat and with much lower energy consumption than CRTs.
			This allowed for entirely new mobile devices like laptops and phones.
		</p>

		<p>
			The latest technology, <strong>light emitting diode</strong> displays, are grids of diodes that individually emit their own light when activated.
			By not requiring a backlight, they can be even thinner and use even less energy, making them practical for even tinier devices, such as smartwatches, head-mounted displays for VR, and other devices with small, battery-powered displays.
		</p>

		<p>
			While all of these advances in display quality might appear only to affect the quality of a picture, they have had dramatic effects on user interfaces.
			For example, none of the mobile devices in use today would be possible with CRT technology.
			Screens would be far too big and far too inefficient to make mobile interaction possible.
			And some sizes of devices, like smartwatches, are only possible with LEDs: their thinness and energy efficiency make room for more battery, which is the critical limiting factor for such small devices.
		</p>

		<p>
			Researchers continue to innovate in display technology, especially with new forms of interaction in mind.
			For example, researchers have enhanced existing display technologies by making them transparent, allowing for new collaboration opportunities while managing privacy (<a href="https://doi.org/10.1145/2642918.2647350">Lindlbauer et al. 2014</a>).
			Others have played with new bendable displays that allow for depth and parallax in images (<a href="https://doi.org/10.1145/2984511.2984524">Gotsch et al. 2016</a>):
		</p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/mGJe0AdszJg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>A bendable 3D parallax display.</center>
		</p>

		<p>
			Other researchers have experimented with projectors, making them track the movement of projection surfaces (<a href="https://doi.org/10.1145/1095034.1095045">Lee et al. 2005</a>), and even allowing projection surfaces to bend (<a href="https://doi.org/10.1145/1449715.1449763">Lee et al. 2008</a>).
			Some have experimented with having multiple users interact with portable projectors, using projection as both a source of output, but also input (<a href="https://doi.org/10.1145/1294211.1294220">Cao et al. 2007</a>).
		</p>

		<p>
			Some have experimented with even smaller displays, such as low-energy displays without batteries (<a href="https://doi.org/10.1145/2984511.2984513">Grosse-Puppendahl et al. 2016</a>) or tiny displays intended to view near eyes (<a href="https://doi.org/10.1145/2642918.2647361">Lyons et al. 2014</a>):
		</p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/0-LZOil6UE8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>A near-eye display.</center>
		</p>

		<p>
			These display innovations promise to lead to entirely new product categories, just as all prior display innovations have removed constraints on the form and function of user interfaces.
		</p>

		<h2>Graphics</h2>

		<p>
			While screens themselves can allow for new form factors, and therefore new experiences, the content on that screen is just as important (if not more).
			One fundamental type of content is computer graphics. Graphics can be anything from basic graphical primitives like lines, rectangles, circles, polygons, and other shapes, to sophisticated simulations like the ones you see in 3D animated movies.
		</p>

		<p>
			Despite all of this complexity, computer graphics have simple foundations.
			Because screens are organized as 2-dimensional arrays of pixels, graphical rendering is all about <strong>coordinate systems</strong>, just like in math.
			There's an x-axis, a y-axis, and at each point in a matrix of pixels, a color.
			Rendering a graphical primitive means specifying a location and color for that shape (and often whether that shape should be filled, or just an outline).
		</p>

		<p>
			<img src="images/shapes.png" class="img-responsive" alt="An assortment of circles, ellipses, rectangles, and polygons of different colors." />
			<center>An assortment of graphical primitives, the building blocks of computer graphics. Credit: Wikimedia.</center>
		</p>

		<p>
			Where this gets interesting is in <em>sequences</em> of graphical operations that are overlaid on top of each other.
			By composing shapes together, and rendering them in a particular order, we can get buttons with backgrounds, scroll bars with depth and texture, windows with drop shadows.
			All of this graphical <strong>compositing</strong> is what allows us to create entire virtual worlds such as desktop interfaces or interactive 3D worlds.
		</p>

		<p>
			Just as important as shapes are images.
			Just like the screen itself, images are represented by 2-dimensional grids of pixels.
			As most computer users know, there are countless ways of storing and compressing this pixel data (bmp, pict, gif, tiff, jpeg, png).
			In the 1980's and 90's, these formats mattered for experience, especially on the web: if you stored pixels in order, uncompressed, an image in a browser would be rendered line by line, as it downloaded, but if you stored it out of order, you could render low-resolution versions of a picture as the entire image downloaded.
			The internet is fast enough today that these format differences don't affect user experience as much.
		</p>

		<p>
			There are many techniques from computer graphics that ensure a high-level of graphical fidelity.
			Color models, transparency, anti-aliasing, sub-pixels, double-buffering, interpolation, GPUs, or other strategies for ensuring precise, high-fidelity, high-performance of <a href="https://en.wikipedia.org/wiki/Computer_graphics">computer graphics</a> ultimately help people focus on content rather than pixels.
			These concepts do become important, however, if you're responsible for the graphic design portion of a user interface.
		</p>

		<center>
			<img src="images/anti-aliasing.jpg" class="img-responsive" alt="A side-by-side comparison of a line with and without antialising, showing smoothing of sharp pixel corners." />
			Anti-aliasing in action. Credit: Wikimedia.
		</center>

		<h2>Typography</h2>

		<p>
			While graphics matter, text might matter more. A huge part of even being able to operate user interfaces are the words we use to explain the semantics of user interface behavior.
			In the early days of command line interfaces, typography was rudimentary: just like screens were grids of pixels, text was presented as a grid of characters.
			This meant that the entire visual language of print, such as fonts, font size, and other dimensions of typography, were fixed and inflexible:
		</p>

		<center>
			<img src="images/monochrome.jpg" class="img-responsive" alt="An IBM PC with a green monochrome screen and a grid of fixed-width font." />
			A monochrome, single font display. Credit: Max Naylor, Wikimedia.
		</center>

		<p>
			Two things changed this.
			First, Xerox PARC, in its envisioning of graphical user interfaces, brought typography to the graphical user interface.
			The conduit for this was primarily its vision of word processing, which attempted to translate the ideas from print to the screen, bringing fonts, font families, font sizes, font weights, font styles, ligature, kerning, baselines, ascents, descents, and other ideas to graphical user interfaces.
			These concepts from typography had been long developed in print, and were directly adapted to screen.
			This required answering questions about how take ideas optimized for ink and paper and translate them to discrete 2-dimensional grids of pixels. Ideas like anti-aliasing and sub-pixel rendering mentioned above, which smooth the harsh edges of pixels, were key to achieving readability.
		</p>

		<center>
			<img src="images/typography.png" class="img-responsive" alt="The word sphinx with annotations of baseline, median, ascent, descent, and other dimensions of typography." />
			Common dimensions of typography. Credit: I, Boffy b (CC-BY-SA).
		</center>

		<p>
			The second decision that helped bring typography to user interfaces was Steve Jobs taking a calligraphy course at Reed College (calligraphy is like typography, but by hand).
			He saw that text could be art, that it could be expressive, and that it was central to differentiating the Mac from the full-text horrors of command lines.
			And so when he saw Xerox PARC's use of typography and envisioned the Mac, type was <a href="https://www.hollywoodreporter.com/news/steve-jobs-death-apple-calligraphy-248900">at the center of his vision</a>.
		</p>

		<center>
			<img src="images/mac-fonts.png" class="img-responsive" alt="The original Mac fonts, including Chicago, Monaco, Geneva, Los Angeles, New York, San Francisco, Toronto, Venice, Geneva, and Chicago." />
			The original Mac fonts. Credit: Unknown.
		</center>

		<p>
			Parallel to these efforts was the need to represent all of the symbols in natural language.
			One of the first standards was ASCII, which represented the Roman characters and Arabic numbers in English, but nothing else.
			<a href="https://unicode.org/">Unicode</a> brought nearly the entire spectrum of symbols and characters to computing, supporting communication within and between every country on Earth.
		</p>

		<p>
			Research on the <em>technology</em> of typography often focuses on readability.
			For example, Microsoft, including researchers from Microsoft Research, developed a sub-pixel font rendering algorithm called ClearType, which they found significantly decreased average reading time (<a href="https://doi.org/10.1145/1124772.1124849">Dillon et al. 2006</a>).
		</p>

		<h2>Data</h2>

		<p>In our modern age of massive data sets, and our keen interest in using that data to answer questions, data visualization has become nearly important as graphics, images, and text. This has provoked questions about how to best render and interact with data sets. The field of data visualization (also known as information visualization) has explored these questions, building upon data visualization efforts in print (<a href=" https://doi.org/10.1007/s00371-013-0892-3">Liu et al. 2014</a>).</p>

		<p>The foundations of data visualization are relatively stable:</p>

		<center>
			<img src="images/viz-pipeline.png" class="img-responsive" alt="A flow from data collection, to data transformation and analsysis, filtering, mapping, rendering, and user interaction through UI controls." />
			The visualization pipeline. Credit: <a href="https://doi.org/10.1007/s00371-013-0892-3">Liu et al. 2014</a>).
		</center>

		<p>Each of these phases has its own interactive complexities. Data transformation often requires interaction in order to "wrangle" data into a structure suitable for visualization (<a href="http://journals.sagepub.com/doi/abs/10.1177/1473871611415994">Kandel et al. 2011)</a>. Filtering involves selecting data for view. Mapping involves taking data values and translating them into things like color, shape, space, size, proximity, and other features of visual information. And then there are a wide array of interaction techniques for seeing relationships between data, including selection, filtering, brushing, linking, focus, and facets.</p>

		<p>Because of the complexity of this pipeline, actually rendering data visualizations has required more direct toolkit support for abstracting away some of the low-level complexities of these phases. Toolkits like <a href="http://mbostock.github.io/protovis/">Protovis</a> (<a href="https://doi.org/10.1109/TVCG.2009.174">Bostock and Heer 2009</a>), <a href="https://d3js.org/">D3</a> (<a href="https://doi.org/10.1109/TVCG.2011.185">Bostock et al. 2011</a>), and <a href="https://vega.github.io/vega/">Vega</a> (<a href="https://doi.org/10.1109/TVCG.2016.2599030">Satyanarauyan et al. 2017</a>) all offer abstractions that reduce this complexity, making it easier to create both static and interactive data visualizations.</p>

		<center>
			<img src="images/d3example.png" class="img-responsive" alt="A grid of factorization visualizations, showing groups and sets of circles of different combinations." />
			A <a href="https://d3js.org/">D3</a> example. Credit: <a href="https://www.jasondavies.com/factorisation-diagrams/">Jason Davies</a>).
		</center>

		<h2>Animation</h2>

		<p>
			While all of the static forms we've discussed above are powerful on their own, efforts to animate these forms offered to increase the expressive power of digital media.
			One early work investigated foundations of animation that might be brought from film animation, including principles of <strong>solidity</strong>, <strong>exaggeration</strong>, and <strong>reinforcement</strong>, which were long used to give life to static images (<a href="https://doi.org/10.1145/168642.168647">Chang and Ungar 1993</a>).
			These principles were tied to specific time-based visual ideas such as arcs, follow-through, slow in/slow out, anticipation, arrivals and departures, and motion blur, all of which are now ubiquitous in things like presentation software and modern graphical user interfaces.
			Just as these types of motion are used in movies to convey information about action, they are now used in user interfaces to convey information, as in this animation in OS X that simulates a head shaking "no":
		</p>

		<p class="embed-responsive embed-responsive-16by9">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/W_WRCMGs1f0?controls=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>A shaking animation, conveying an incorrect password.</center>
		</p>

		<p>
			While this basic idea of animating interfaces was straightforward, finding ways to seamlessly implement animation into interfaces was not.
			Having parts of interfaces move required careful management of the position of interface elements over time, and these were incompatible with the notion of view hierarchies determining element positions at all times.
			Some of the earliest ideas involved defining <strong>constraints</strong>, and letting those constraints determine position over time (<a href="https://doi.org/10.1145/237091.237109">Myers et al. 1996</a>). For example, a developer might say that an element should be at position A at time t and then at position B at time t+1, and then let the user interface toolkit decide precisely where to render the element between those two times.
			This same idea could be used to animate any visual property of an element, such as its color, transparency, size, and so on.
			These same ideas eventually led to more sophisticated animations of direct manipulation interfaces (<a href="https://doi.org/10.1145/215585.215628">Thomas and Calder 1995</a>), of icons (<a href="https://doi.org/10.1145/1978942.1979232">Harrison et al. 2011</a>), and of typography (<a href="https://doi.org/10.1145/571985.571997">Lee et al. 2002</a>).
			These ideas coalesced into well-defined transition abstractions that made it easier to express a range of "emotions" through transitions, such as urgency, delay, and confidence (<a href="https://doi.org/10.1145/168642.168648">Hudson and Stasko 1993</a>).
		</p>

		<p>
			All of these research ideas are now ubiquitous in toolkits like Apple's <a href="https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/CoreAnimation_guide/Introduction/Introduction.html">Core Animation</a>, which make it easy to express animation states without having to manage low-level details of user interface rendering.
		</p>

		<hr>

		<p>While the 2000's saw 3D begin to dominate games and movies, 2D rendering is still at the heart of interactions with user interfaces. While much of the research in this space has moved on to interactive 3D experiences, the foundations built over the past fifty years remain with us, sitting out the foundation of operating systems, graphic design, and the internet.</p>

		<center><p class="lead"><a href="3D.html">Next chapter: 3D output</a></p></center>

		<h2>Further reading</h2>

		<p>Michael Bostock and Jeffrey Heer 2009. <a href="https://doi.org/10.1109/TVCG.2009.174">Protovis: A Graphical Toolkit for Visualization</a>. IEEE Transactions on Visualizations and Computer Graphics. pdf</p>

		<p>Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. <a href="https://doi.org/10.1109/TVCG.2011.185">D³ data-driven documents</a>. IEEE transactions on visualization and computer graphics 17, no. 12 (2011): 2301-2309.</p>

		<p>Xiang Cao, Clifton Forlines, and Ravin Balakrishnan. 2007. <a href="https://doi.org/10.1145/1294211.1294220">Multi-user interaction using handheld projectors</a>. In Proceedings of the 20th annual ACM symposium on User interface software and technology (UIST '07). ACM, New York, NY, USA, 43-52.</p>

		<p>Bay-Wei Chang and David Ungar. 1993. <a href="https://doi.org/10.1145/168642.168647">Animation: from cartoons to the user interface</a>. In Proceedings of the 6th annual ACM symposium on User interface software and technology (UIST '93). ACM, New York, NY, USA, 45-55.</p>

		<p>Andrew Dillon, Lisa Kleinman, Gil Ok Choi, and Randolph Bias. 2006. <a href="http://dx.doi.org/10.1145/1124772.1124849">Visual search and reading tasks using ClearType and regular displays: two experiments</a>. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '06), Rebecca Grinter, Thomas Rodden, Paul Aoki, Ed Cutrell, Robin Jeffries, and Gary Olson (Eds.). ACM, New York, NY, USA, 503-511.</p>

		<p>Chris Harrison, Gary Hsieh, Karl D.D. Willis, Jodi Forlizzi, and Scott E. Hudson. 2011. <a href="https://doi.org/10.1145/1978942.1979232">Kineticons: using iconographic motion in graphical user interface design</a>. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). ACM, New York, NY, USA, 1999-2008.</p>

		<p>Tobias Grosse-Puppendahl, Steve Hodges, Nicholas Chen, John Helmes, Stuart Taylor, James Scott, Josh Fromm, and David Sweeney. 2016. Exploring the Design Space for Energy-Harvesting Situated Displays. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 41-48.</p>

		<p>Daniel Gotsch, Xujing Zhang, Juan Pablo Carrascal, and Roel Vertegaal. 2016. <a href="https://doi.org/10.1145/2984511.2984524">HoloFlex: A Flexible Light-Field Smartphone with a Microlens Array and a P-OLED Touchscreen</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 69-79.</p>

		<p>Scott E. Hudson and John T. Stasko. 1993. <a href="https://doi.org/10.1145/168642.168648">Animation support in a user interface toolkit: flexible, robust, and reusable abstractions</a>. In Proceedings of the 6th annual ACM symposium on User interface software and technology (UIST '93). ACM, New York, NY, USA, 57-67.</p>

		<p>Sean Kandel, Jeffrey Heer, Catherine Plaisant, Jessie Kennedy, Frank van Ham, Nathalie Henry Riche, Chris Weaver, Bongshin Lee, Dominique Brodbeck, and Paolo Buono. 2011. <a href="http://journals.sagepub.com/doi/abs/10.1177/1473871611415994">Research directions in data wrangling: Visualizations and transformations for usable and credible data</a>. Information Visualization 10, no. 4: 271-288.</p>

		<p>Johnny C. Lee, Scott E. Hudson, Jay W. Summet, and Paul H. Dietz. 2005. <a href="http://dx.doi.org/10.1145/1095034.1095045">Moveable interactive projected displays using projector based tracking</a>. In Proceedings of the 18th annual ACM symposium on User interface software and technology (UIST '05). ACM, New York, NY, USA, 63-72.</p>

		<p>Johnny C. Lee, Scott E. Hudson, and Edward Tse. 2008. <a href="https://doi.org/10.1145/1449715.1449763">Foldable interactive displays</a>. In Proceedings of the 21st annual ACM symposium on User interface software and technology (UIST '08). ACM, New York, NY, USA, 287-290.</p>

		<p>David Lindlbauer, Toru Aoki, Robert Walter, Yuji Uema, Anita Höchtl, Michael Haller, Masahiko Inami, and Jörg Müller. 2014. <a href="https://doi.org/10.1145/2642918.2647350">Tracs: transparency-control for see-through displays</a>. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). ACM, New York, NY, USA, 657-661.</p>

		<p>Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. 2017. <a href="https://doi.org/10.1109/TVCG.2016.2599030">Vega-lite: A grammar of interactive graphics</a>. IEEE Transactions on Visualization and Computer Graphics 23, no. 1 (2017): 341-350.</p>

		<p>Liu, Shixia, Weiwei Cui, Yingcai Wu, and Mengchen Liu. 2014. <a href="https://doi.org/10.1007/s00371-013-0892-3">A survey on information visualization: recent advances and challenges</a>. The Visual Computer 30, no. 12 (2014): 1373-1393.</p>

		<p>Kent Lyons, Seung Wook Kim, Shigeyuki Seko, David Nguyen, Audrey Desjardins, Mélodie Vidal, David Dobbelstein, and Jeremy Rubin. 2014. <a href="https://doi.org/10.1145/2642918.2647361">Loupe: a handheld near-eye display</a>. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). ACM, New York, NY, USA, 351-354.</p>

		<p>Brad A. Myers, Robert C. Miller, Rich McDaniel, and Alan Ferrency. 1996. <a href="http://dx.doi.org/10.1145/237091.237109">Easily adding animations to interfaces using constraints</a>. In Proceedings of the 9th annual ACM symposium on User interface software and technology (UIST '96). ACM, New York, NY, USA, 119-128.</p>

		<p>Sito, T. (2013). Moving innovation: a history of computer animation. MIT Press.</p>

		<p>Bruce H. Thomas and Paul Calder. 1995. <a href="http://dx.doi.org/10.1145/215585.215628">Animating direct manipulation interfaces</a>. In Proceedings of the 8th annual ACM symposium on User interface and software technology (UIST '95). ACM, New York, NY, USA, 3-12.</p>

		<script type="text/javascript">
		
			var _gaq = _gaq || [];
			_gaq.push(['_setAccount', 'UA-10917999-1']);
			_gaq.push(['_trackPageview']);
			
			(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
			})();
		
		</script>

	</body>

</html>
